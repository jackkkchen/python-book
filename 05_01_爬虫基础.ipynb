{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"爬虫基础\"\n",
    "format:\n",
    "  html:\n",
    "  #  code-fold: true\n",
    "   code-tools: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬虫的基本概念\n",
    "\n",
    "爬虫，即一段自动抓取互联网信息的程序，从互联网上抓取对于我们有价值的信息。\n",
    "\n",
    "根据爬虫的定义，很容易联想到各类浏览器、搜索引擎，我们日常生活中使用浏览器搜索网站，也可以看作是在使用爬虫爬取信息。只不过，浏览器的搜索结果往往是一整个网页，而我们常常只需要网页中特定的内容。比如，当我们想要获取当前电影市场上评分最高的 250 部电影，我们可以使用浏览器进行搜索。\n",
    "\n",
    "![电影 TOP250](img/电影TOP250.png)\n",
    "\n",
    "从图片中我们可以发现，除了我们想要的电影的信息，还有许多不相干的元素，比如APP的广告信息等等，因此，我们可以尝试使用爬虫，来获取我们想要的、存粹的电影榜单TOP250。\n",
    "\n",
    "为了实现我们的目的，首先，我们要来了解，什么是爬虫。\n",
    "\n",
    "### 一、爬虫的概念\n",
    "\n",
    "Python爬虫是一种自动化程序，设计用于在互联网上浏览网页并提取信息。它模仿人类在网页上浏览的行为，但可以在短时间内处理大量的网页和数据。Python爬虫通常用于从网页中抓取数据、分析信息、搜索特定内容等。\n",
    "\n",
    "根据前面学到的知识，我们可以简单的讲爬虫理解为一段自动化的代码，通过各式条件判断、循环来从网页上抓取我们想要的内容，它模拟人类的操作，向服务器发送请求，获取数据。因此，要使用爬虫，就要先了解如何与服务器建立连接，使用何种传输协议。同时，由于服务器返回的的内容通常不是整理好的文档或表格，而是 HTML 文档，因此我们也需要了解 HTML 文档的构成，以帮助我们了解如何从网页上抓取我们想要的部分，或者如何处理抓取到的数据。\n",
    "\n",
    "这样一来，我们似乎整理出了爬虫的基本运行方式，即：\n",
    "\n",
    "1. 请求库(HTTP 请求)：首先，爬虫使用 HTTP 请求库（如 `requests`）向目标网站发送请求，获取网页的HTML源代码。这一步骤获取了需要被处理的网页内容。\n",
    "2. 网页解析：接下来，获取到的 HTML 源代码需要被解析以从中提取有用的信息。这可以通过使用 HTML 解析库（如 `Beautiful Soup`、`lxml` 等）或者正则表达式等方法来完成。解析后的数据可以更方便地进行后续的处理和提取。\n",
    "3. 数据提取：在解析的基础上，爬虫可以使用解析库提供的工具（如标签解析、XPath、CSS 选择器）来从 HTML 源代码中提取所需的数据。这些数据可以是网页的标题、段落、链接、图像等等。\n",
    "4. 数据存储：最后，提取到的数据可以被存储在本地文件、数据库或其他的数据存储方式中，以备后续的分析、使用或展示。这一步骤使得爬虫获得的有用信息可以被进一步处理和利用。\n",
    "\n",
    "根据爬虫的基本运行方式，我们也就明了了学习爬虫的步骤。首先，我们需要了解什么是 HTTP 请求，什么是 HTML 文档。然后，我们就可以学习使用 HTTP 请求库来获取网页的基本数据。最后，我们再掌握如何对网页数据进行解析并从中提取我们所需要的数据。\n",
    "\n",
    "另外，为了简化爬虫的开发过程，使得我们可以更高效地从网页中提取所需要的信息，我们还需要学习爬虫框架。\n",
    "\n",
    "爬虫框架就是一种开发和执行爬虫任务的工具，提供了一种结构化的方法来组织代码，并处理诸如请求、解析、存储、并发等方面的复杂问题。\n",
    "\n",
    "使用爬虫框架的好处包括：\n",
    "\n",
    "- 高级功能： 爬虫框架提供了许多高级功能，如自动处理请求、响应、解析、数据存储等。这使开发者能够更专注于数据提取和分析，而不必从头开始实现每个细节。\n",
    "- 并发处理： 爬虫框架通常支持并发处理，能够同时发出多个请求并处理多个响应。这有助于提高爬取效率，节省时间。\n",
    "- 自动重试和错误处理： 框架能够自动处理一些网络错误、连接超时等问题，并支持自动重试机制，从而减少因网络问题而导致的爬取中断。\n",
    "- 解析器集成： 许多框架内置了解析器，用于解析HTML或XML等格式的文档。这使得数据提取过程更加方便。\n",
    "- 数据存储： 框架通常提供数据存储的选项，例如将数据保存到数据库、CSV文件或其他存储介质中。\n",
    "- 分布式爬取： 一些框架支持分布式爬取，允许多台机器同时工作，加速数据收集过程。\n",
    "- 可配置性： 框架通常允许你对爬虫的行为进行配置，如请求间隔、User-Agent设置等。\n",
    "- 错误处理和日志记录： 框架提供了错误处理和日志记录机制，使得开发者能够追踪爬虫运行中的问题。\n",
    "\n",
    "了解了爬虫的基本概念后，我们就可以为接下来的学习做一个初步的规划了。\n",
    "\n",
    "首先，我们将会学习 HTTP&HTTPS 与 HTML 的概念，并且了解如何从一个网页中找到我们所需要的信息。\n",
    "\n",
    "随后，我们学习 `requests` 库，这是一个简洁而功能强大的Python库，用于发送HTTP请求和处理HTTP响应。它使得在Python中进行网络请求变得更加简单，提供了一种直观的方式来与Web服务进行交互。\n",
    "\n",
    "最后，我们学习 `scrapy` 框架，它是一个高级的、开源的Python爬虫框架，专门设计用于快速、高效地从网站中提取结构化数据。它提供了一整套工具和抽象层，使得开发者能够轻松地创建、管理和执行各种类型的网络爬虫任务。\n",
    "\n",
    "### 二、爬虫策略与爬虫伦理\n",
    "\n",
    "在正式开始爬虫的学习之前，我们还有一个十分重要的任务要做，就是了解爬虫策略与爬虫伦理。\n",
    "\n",
    "首先我们要了解，爬虫作为一种技术手段，本身是不违法任何法律条文的，但是开发者使用爬虫进行网络数据获取的行为可能会涉及到法律风险。这是因为，虽然许多网站向用户免费提供服务，放开了数据获取权限，但是网站的数据依然是受到版权或知识产权保护的，随意使用爬虫不仅会触犯到网站所有者的权益，也可能危害到数据隐私和个人信息。另外，网站的运营和维护都是需要成本的，网站的服务器资源也是有限的，如果不受限制地使用爬虫，会加重网站的运行负担，占用过多的服务器资源，甚至无异于 DDoS 攻击，这会导致网站服务器过载从而无法正常提供服务，也会影响到其他用户的使用。\n",
    "\n",
    "使用爬虫的法律风险包括但不限于：\n",
    "\n",
    "- 侵犯版权和知识产权： 爬虫可能会获取并使用受版权保护的内容，如文本、图片、音频等。在未经授权的情况下使用这些内容可能构成版权侵犯，导致法律纠纷。\n",
    "- 数据隐私和个人信息： 爬虫获取的数据可能包含个人隐私信息，如姓名、邮箱、电话号码等。在处理这些信息时，需要遵守相关数据隐私法规，避免侵犯他人隐私权。\n",
    "- 反竞争行为： 如果使用爬虫获取竞争对手的商业信息，可能被视为不正当竞争行为，可能触发法律纠纷。\n",
    "- 网络犯罪： 如果爬虫被用于非法活动，如网络钓鱼、诈骗等，可能构成网络犯罪，导致刑事责任。\n",
    "- 违反网站条款： 有些网站在其服务条款中规定了爬虫行为的限制，如果不遵守这些规定，可能导致法律风险。\n",
    "- 分布式拒绝服务攻击（DDoS）： 如果爬虫请求过于频繁，可能会对网站服务器造成压力，被视为DDoS攻击，导致法律追究。\n",
    "- 机器人协议（robots.txt）： 忽略网站的robots.txt文件，即使不是法律违规，也可能导致网站封禁你的爬虫，影响数据获取。\n",
    "- 法律地域差异： 不同国家和地区对爬虫活动的法律规定不同，需要根据实际情况遵循当地法律法规。\n",
    "\n",
    "基于以上因素，在学习爬虫之前，我们首先要了解爬虫策略与爬虫伦理。\n",
    "\n",
    "#### 1. 爬虫策略\n",
    "\n",
    "爬虫策略是指开发者在进行网络爬虫任务时制定的计划和方法。它涉及爬取的目标、深度、频率、数据提取、并发等方面的决策，以确保爬虫能够高效、有序地从目标网站抓取数据。合理的爬虫策略可以降低对目标网站的负担，减少被封禁的风险，同时也能够保证数据的准确性和完整性。\n",
    "\n",
    "爬虫策略的一些重要考虑因素包括：\n",
    "\n",
    "- 爬取目标： 确定要爬取的网站或页面，明确所需的数据。\n",
    "- 爬取深度： 决定爬虫深入网站的层级，即爬取多少个链接的链接。\n",
    "- 爬取频率： 设定请求频率，避免对网站造成过大的压力，防止被封禁。\n",
    "- 数据提取： 确定如何从网页中提取所需的数据，使用选择器、正则表达式等方法。\n",
    "- 请求头设置： 设置适当的User-Agent、Referer等请求头，模拟正常用户行为。\n",
    "- 并发处理： 考虑如何有效地处理多个并发请求，提高爬取效率。\n",
    "- 错误处理： 处理网络错误、连接超时等问题，设置重试机制。\n",
    "\n",
    "设置合理的爬虫策略，不仅有利于进行更高效更准确的数据爬取，也有利于保护自身不触犯法律风险。\n",
    "\n",
    "#### 2. 爬虫伦理\n",
    "\n",
    "为了设置合理的爬虫策略，我们就需要了解爬虫伦理。\n",
    "\n",
    "爬虫伦理是指在进行网络爬虫任务时应遵循的道德和法律规范。因为爬虫可能会对目标网站产生一定的影响，开发者应该遵循以下几个伦理原则：\n",
    "\n",
    "- 尊重网站规则： 开发者应该遵循目标网站的 robots.txt 文件，避免爬取被禁止的页面。\n",
    "- 避免过度请求： 爬虫不应该对目标网站造成不必要的负担，应该合理设置请求频率和间隔。\n",
    "- 尊重隐私： 爬虫不应该获取或传播个人隐私信息，如邮箱、密码等。\n",
    "- 遵循法律法规： 爬虫开发者应该遵守国家和地区的相关法律法规，避免侵犯他人权益。\n",
    "- 避免滥用： 爬虫不应该被用于恶意目的，如大规模数据盗取、DDoS攻击等。\n",
    "- 保护目标网站： 爬虫应该避免对目标网站造成影响，如不要频繁爬取大量页面。\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "**robots.txt 文件**\n",
    "\n",
    "robots.txt 文件是一个用于指导网络爬虫如何爬取网站内容的标准文件。这个文件通常位于网站的根目录下，我们可以通过在网站域名后面添加 /robots.txt 来访问。\n",
    "\n",
    "robots.txt 文件的主要作用是告诉网络爬虫哪些页面可以爬取，哪些页面不应该被爬取。它是网站所有者与搜索引擎和其他爬虫之间的一种协议，有助于管理网络爬虫的行为，以确保爬虫不会访问或爬取敏感、私人或不希望被索引的内容。\n",
    "\n",
    "我们可以打开任意一个网站，在其域名后添加  来访问其 robots.txt 文件。下面是一个简单的 robots.txt 文件示例：\n",
    "\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /private/\n",
    "Disallow: /admin/\n",
    "Allow: /public/\n",
    "Crawl-delay: 10\n",
    "```\n",
    "\n",
    "这个文件的结构为：\n",
    "\n",
    "User-agent：这个部分指定了要针对哪个爬虫或用户代理设置规则。通常使用一个星号 * 来表示适用于所有爬虫，或者指定爬虫的名称或代号。\n",
    "Disallow：这个部分规定了爬虫不允许访问的页面或目录。例如，Disallow: /private/ 表示不允许爬虫访问以 /private/ 开头的所有页面。\n",
    "Allow：与Disallow相对应，这个部分规定了允许访问的页面或目录。通常不是所有的爬虫都支持这个指令。\n",
    "Crawl-delay：这个部分指定了爬虫爬取网站页面的延迟时间，以秒为单位。例如，Crawl-delay: 5 表示爬虫每隔5秒才能请求一个页面。\n",
    "\n",
    ":::\n",
    "\n",
    "总之，爬虫策略和伦理是网络爬虫开发中不可忽视的重要方面。通过合理的策略和遵循伦理原则，开发者可以实现有效的数据抓取，同时避免对网站和其他用户造成不必要的困扰和损害。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP & HTTPS 与 HTML\n",
    "\n",
    "HTTP 协议是客户端与服务器之间通讯所遵循的协议，要学习爬虫，就要首先了解，客户端如何与服务器建立连接，客户端与服务器之间如何互相传递信息。\n",
    "\n",
    "### 一、 HTTP & HTTPS\n",
    "\n",
    "#### 1. HTTP & HTTPS 的概念\n",
    "\n",
    "首先让我们一起来看看，什么是 HTTP 协议。\n",
    "\n",
    "HTTP(Hypertext Transfer Protocol)即超文本传输协议，是一种用于在客户端和服务器之间传输数据的请求-响应协议。在我们的日常生活中，HTTP 协议最常使用的地方就是在浏览器中浏览网页。我们在浏览器的地址栏中输入一个网址，浏览器就会向该网站的服务器发送一个请求，然后等待服务器返回给浏览器的回应。\n",
    "\n",
    "HTTP 是 Web 应用程序和互联网上的服务器之间通信的基础。HTTP 被设计用于传输超文本，通常指 HTML 文档，但也可以传输其他类型的数据，如图像、音频、视频等。\n",
    "\n",
    "HTTPS(Hypertext Transfer Protocol Secure)是基于 HTTP 提出的**安全**超文本传输协议，即 HTTP 的安全版本。HTTPS 旨在保护网络通信的隐私和安全性，它通过多种加密方式来加密数据传输，防止敏感信息在传输过程中被恶意窃听或篡改。\n",
    "\n",
    "关于 HTTPS，需要注意的点是：\n",
    "\n",
    "- 端口：默认情况下，HTTPS 使用 TCP 端口443，而 HTTP 使用端口80。\n",
    "- URL前缀：HTTPS 网站的 URL 以 \"https://\" 开头，而 HTTP 网站以 \"http://\" 开头。这个前缀是浏览器的一种提示，告诉用户连接是安全的。\n",
    "- 身份验证：HTTPS 可以通过数字证书对服务器进行身份验证，确保客户端正在连接到合法的服务器，而不是中间人攻击者。数字证书由可信的证书颁发机构（CA）颁发。\n",
    "- 数据完整性：HTTPS 还提供了数据完整性检查，以确保在传输过程中数据没有被篡改或损坏。数据完整性是通过使用消息摘要算法（如SHA-256）和数字签名来实现的。\n",
    "\n",
    "::: {.callout-note}\n",
    "在学习 HTTP 时，我们还需要了解 URI 的概念。\n",
    "\n",
    "**URI(Uniform Resource Identifier)：**\n",
    "\n",
    "URI是一个通用的概念，它的中文名称是“统一资源标识符”，它用于标识和定位任何资源，不仅仅是互联网上的资源。URI 分为两种形式：URL 和 URN。\n",
    "\n",
    "URL(Uniform Resource Locator)： URL 是 URI 的一个子集，他就是我们常说的“统一资源定位符”，它包含了资源的地址以及获取该资源所需的协议信息。URL 通常以特定的协议（如HTTP、HTTPS、FTP等）开头，后面跟着资源的地址和可能的其他信息。例如，https://www.example.com/index.html 是一个URL，它包括了协议（HTTPS）、主机名（www.example.com）以及资源路径（/index.html）。\n",
    "\n",
    "URN（Uniform Resource Name）： URN 是 URI 的另一种子集，又叫做“统一资源名称”，它用于标识资源的名称，而不涉及资源的位置或如何获取资源。URN 的一个例子是 ISBN（国际标准书号），它标识了一本书的唯一名称，而不管它存储在哪里。\n",
    "\n",
    "#### 2. HTTP & HTTPS 的请求方法\n",
    "\n",
    "HTTP 具有两种常见的请求方法：`Get` 和 `Post`。其中，`Get` 方法主要用于获得数据，比如我们浏览网页时，浏览器就会向服务器发送 `Get` 请求，用以获取页面数据。`Post` 方法主要用于创建数据，比如当我们注册账号时，浏览器就会向服务器发送 `Post` 请求，将用户数据传递给服务器。\n",
    "\n",
    "由于本次课程主要是讲解爬虫程序的基本概念，因此后面我们发送的请求大多会使用 `Get` 方法。\n",
    "\n",
    "一个 HTTP 请求由请求行、请求头、请求体三部分组成。现在，让我们一起来看看一个完整的 HTTP 请求长什么样。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "GET /index.html HTTP/1.1<!-- 请求行(Request Line) -->\n",
    "Host: www.example.com<!-- 请求头(Request Headers) -->\n",
    "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0\n",
    "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\n",
    "Accept-Language: en-US,en;q=0.5\n",
    "Connection: keep-alive\n",
    "<!-- 空行 -->\n",
    "<!-- 请求体(Request Body)-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，HTTP请求的结构如下：\n",
    "\n",
    "- 请求行（Request Line）： 包括请求方法（`GET`）、资源路径（`/index.html`）和协议版本（`HTTP/1.1`）。\n",
    "\n",
    "- 请求头部（Request Headers）： 包括多个头部字段，每个字段用冒号分隔，比如 `Host`、`User-Agent`、`Accept` 等。这些头部字段描述了请求的细节和附加信息。\n",
    "\n",
    "- 空行（Blank Line）： 空行用于分隔请求头部和请求体。\n",
    "\n",
    "- 请求体（Request Body）： `GET` 请求通常没有请求体，因此此处为空。\n",
    "\n",
    "> 1. 请求行中的资源路径指明了客户端想要访问的服务器中的资源，在资源路径后面，还可以填入一个 `?`，用以添加查询参数，用以向服务器传递额外的信息，不同的信息之间用 `&` 分隔。如：\n",
    "> \n",
    "> `www.douban.com/movie/top250?start=750&filter=unwatched`\n",
    ">\n",
    "> 2. 请求头会包含一些传递给服务器的信息，其中：\n",
    ">   (1) `Host` 指主机域名，主机域名结合请求行里面的资源路径，就可以得到一个完整的网址\n",
    ">   (2) `User-Agent` 用来告知服务器客户端的相关信息，比如请求发起者的身份等等\n",
    ">   (3) `Accept` 告诉服务器客户端希望接受的响应数据的类型，多种类型以 `,` 分隔， `*/*` 表示接受任意类型的数据\n",
    ">\n",
    "> 3. 请求体一般包含客户端想要传递给服务器的其他任意数据\n",
    "\n",
    "当服务器接收到客户端发出的 HTTP 请求后，会根据请求返回 HTTP 响应。类似的，HTTP 响应也由三个部分组成：状态行、响应头、响应体。观察下面的例子，这是一个完整的 HTTP 响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "HTTP/1.1 200 OK<!-- 状态行(Status Line) -->\n",
    "Date: Sat, 28 Aug 2023 12:00:00 GMT<!-- 响应头(Response Headers) -->\n",
    "Server: Apache/2.4.41 (Unix)\n",
    "Content-Type: text/html; charset=UTF-8\n",
    "Content-Length: 1234\n",
    "Connection: keep-alive\n",
    "\n",
    "<!DOCTYPE html><!-- 响应体(Response Body) -->\n",
    "<html>\n",
    "<head>\n",
    "    <title>Welcome to Example.com</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Hello, World!</h1>\n",
    "    <p>This is an example HTML page.</p>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个实际的HTTP响应示例中：\n",
    "\n",
    "- 状态行（Status Line）： `HTTP/1.1 200 OK` 表示服务器的响应状态，其中 \"`200 OK`\" 是状态码和状态描述。\n",
    "\n",
    "- 响应头部（Response Headers）： 头部字段包括了响应的详细信息：\n",
    "\n",
    "  - `Date`： 表示响应发送的日期和时间。\n",
    "  - `Server`： 表示响应的服务器信息。\n",
    "  - `Content-Type`： 表示响应内容的类型，这里是 HTML。\n",
    "  - `Content-Length`： 表示响应体的长度，这里是 1234 字节。\n",
    "  - `Connection`： 表示连接的保持方式，这里是保持连接。\n",
    "\n",
    "- 空行（Blank Line）： 空行用于分隔响应头部和响应体。\n",
    "\n",
    "- 响应体（Response Body）： 响应体包含了服务器返回的 HTML 内容，即页面的结构和内容。\n",
    "\n",
    "状态码表示了服务器对客户端的请求的响应状态，共分为五类：\n",
    "\n",
    "1. `1xx`(信息性状态码)：这些状态码是信息性的，表示请求已被接收，继续处理。最常见的1xx状态码是100 Continue，表示服务器已经接收到请求的一部分，客户端可以继续发送请求的其余部分。\n",
    "2. `2xx`(成功状态码)：这些状态码表示请求已成功接收、理解和接受。最常见的2xx状态码是200 OK，表示请求成功。\n",
    "3. `3xx`(重定向状态码)：这些状态码表示客户端需要采取额外的操作来完成请求。通常用于指示资源已经移动到新的URL，需要重新发起请求。常见的3xx状态码包括`301 Moved Permanently`（永久重定向）和`302 Found`（临时重定向）。\n",
    "4. `4xx`(客户端错误状态码)：这些状态码表示客户端发送的请求有错误或无法完成。最常见的4xx状态码是`400 Bad Request`（请求无效）和`403 Forbidden`（禁止访问）。这些状态码通常表示客户端需要修改请求以使其有效。\n",
    "5. `5xx`(服务器错误状态码)：这些状态码表示服务器在尝试处理请求时发生了错误。最常见的5xx状态码是`500 Internal Server Error`（服务器内部错误）。这些状态码通常表示服务器出现问题，无法处理请求。\n",
    "\n",
    "下面的表格中给出了常见的 HTTP 状态码，及其含义：\n",
    "\n",
    "| 状态码 | 含义                             |\n",
    "|--------|----------------------------------|\n",
    "| 200 OK | 客户端请求成功                   |\n",
    "| 201 Moved Permanently | 资源被永久移动到新地址    |\n",
    "| 400 Bad Request | 客户端不能被服务器所理解    |\n",
    "| 401 Unauthorized | 请求未经授权             |\n",
    "| 403 Forbidden | 服务器拒绝提供服务         |\n",
    "| 404 Not Found | 请求资源不存在           |\n",
    "| 500 Internal Server Error | 服务器发生不可预期的错误 |\n",
    "| 503 Service Unavailable | 服务器当前不能处理客户端的请求 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、HTML\n",
    "\n",
    "#### 1. HTML 的概念\n",
    "\n",
    "一个网页通常由以下三大技术要素组成：\n",
    "\n",
    "- `CSS`：定义网页的样式\n",
    "- `HTML`：定义网页的结构和信息\n",
    "- `JavaScript`：定义用户和网页的交互逻辑\n",
    "\n",
    "由于我们在爬取数据时，主要关心的是网页上面的信息，因此让我们首先来看看什么是 HTML。\n",
    "\n",
    "HTML(HyperText Markup Language)即超文本标记语言，是一种用于创建和组织网页内容的标记语言。它被广泛应用于 Web 开发中，是互联网上最基本的构建块之一，用于描述网页上的文本、图像、链接、表格、多媒体和其他元素的结构和外观。HTML使用标记（标签）来定义文档的结构和内容，这些标记可以通过Web浏览器解释和渲染，以便用户查看和与之交互。\n",
    "\n",
    "在学习 HTML 文档时，我们首先要记住的概念就是，HTML 是一种**标记语言**，它使用一系列的**标签**(由尖括号 `<>` 括起来)来标记文档的各个部分，如标题、段落、图像等。\n",
    "\n",
    "::: {.callout-note}\n",
    "**文本与超文本**\n",
    "\n",
    "(1) 文本（Text）\n",
    "- 文本是一种线性的、有序的信息传递方式。它通常指代书写、打印或输入的字母、数字和符号组成的字符串。\n",
    "- 文本只包含基本字符，没有多样的格式或媒体元素。\n",
    "- 在计算机领域，文本通常指代不包含多媒体内容、链接或其他互动元素的字符数据。\n",
    "\n",
    "(2) 超文本（Hypertext）\n",
    "- 超文本是一种非线性的、互动的信息传递方式。它不仅包含文本内容，还包括可以链接到其他文档、网页、图像、视频等媒体的超链接。\n",
    "- 超文本具有更丰富的表现形式，可以通过链接实现跳转、导航和互动。\n",
    "- 在计算机领域，超文本通常指代包含超链接的内容，这些超链接可以将用户引导到相关的信息或资源。\n",
    "\n",
    "总的来说，文本强调线性、顺序式的信息传递，而超文本强调非线性、互动式的信息传递，通过链接实现了内容的关联性和丰富性。Web 中的超文本是基于 HTML 构建的，通过超链接和其他元素，使用户能够在不同的网页和资源之间进行自由导航和交互。\n",
    "\n",
    ":::\n",
    "\n",
    "让我们一起来看看一个简单的 HTML 示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "\n",
    "<head>\n",
    "    <title>简单的网页示例</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <h1>欢迎来到我的网页</h1>\n",
    "    <p>这是一个简单的HTML示例，用于展示基本的网页结构和元素。</p>\n",
    "    <p>你可以点击下面的链接跳转到其他网页：</p>\n",
    "    <ul>\n",
    "        <li><a href=\"https://python.pypandas.com/\">示例链接1</a></li>\n",
    "        <li><a href=\"https://movie.douban.com/top250\">示例链接2</a></li>\n",
    "    </ul>\n",
    "</body>\n",
    "\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细观察示例，你会发现许多由尖括号 `<>` 包裹起来的内容，这就是 HTML 文档的**标签**。\n",
    "\n",
    "从这个示例中，我们可以学习到 HTML 文档的结构，HTML 文档通常由以下几个基本部分组成：\n",
    "\n",
    "- `<!DOCTYPE>` 声明：定义文档的类型和版本，指示使用HTML5标准。\n",
    "- `<html>`：文档的根元素，包含了整个HTML文档。\n",
    "- `<head>`：包含文档的元信息，如标题、字符编码和链接到外部资源的信息。\n",
    "- `<title>`：定义文档的标题，显示在浏览器的标题栏或标签页上。\n",
    "- `<body>`：包含文档的主要内容，如文本、图像、链接等。\n",
    "\n",
    "HTML 中的标签用于定义文档的结构和内容。标签通常成对出现，包括起始标签和闭合标签。例如，`<p>` 是表示段落的起始标签，`</p>` 是表示段落的闭合标签。标签之间的内容就 称为元素。\n",
    "\n",
    "::: {.callout-note}\n",
    "HTML 文档由许许多多的标签组成，每个标签用于定义不同的功能，下面是一些常见的 HTML 标签：\n",
    "\n",
    "- 元素（Elements）： 标签及其内容一起被称为元素。元素可以包含文本、其他标签、属性等。\n",
    "- 属性（Attributes）： 标签可以具有属性，属性提供了关于元素更多的信息，如样式、链接目标等，例如，`<img>` 元素通常包含 `src` 属性，用于指定图像的来源。属性总是在开始标签中定义，并以键值对的形式表示，如 `attribute=\"value\"`。\n",
    "- 文档结构： HTML文档通常包含 `<!DOCTYPE>`、`<html>`、`<head>`、`<title>`、`<meta>`、`<body>` 等标签，用于定义文档类型、头部信息和主体内容。HTML 允许标签嵌套，即在一个元素内部包含另一个元素。嵌套的元素必须正确打开和关闭，以确保文档的结构正确。\n",
    "- 标题： 使用 `<h1>`、`<h2>`、`<h3>` 等标题标签定义页面的标题，标题按重要性逐级递减。\n",
    "- 段落： 使用 `<p>` 标签定义段落，用于包含文本段落。\n",
    "- 链接： 使用 `<a>` 标签定义链接，可以链接到其他页面、文件、位置等，我们日常使用的超链接，就是通过 `<a>` 标签定义的。\n",
    "- 图像： 使用 `<img>` 标签插入图像，需要指定图像的来源（URL）等信息。\n",
    "- 列表： 使用 `<ul>`（无序列表）和 `<ol>`（有序列表）定义列表，使用 `<li>` 定义列表项。\n",
    "- 表格： 使用 `<table>` 定义表格，使用 `<tr>` 定义表格行，使用 `<td>` 定义单元格。\n",
    "- 表单： 使用 `<form>` 定义表单，用于用户输入和提交数据。包含输入字段、按钮等。\n",
    "\n",
    "HTML是Web开发的基础，它为浏览器提供了构建网页结构和内容的方式。通过将不同的HTML元素组合在一起，开发者可以创建丰富多样的网页，从简单的文本页面到复杂的互动应用。\n",
    "\n",
    ":::\n",
    "\n",
    "在上面的 HTML 示例中，仔细观察标签 `<body>` 中的内容，这就是它的子元素。在 HTML 和 CSS 中，元素之间存在多种关系，其中两个重要的关系是“父子元素”和“兄弟元素”。\n",
    "\n",
    "- 父子元素关系： 父子元素关系指的是一个元素包含在另一个元素的内部，形成了层级关系。在 HTML 中，这通常表示一个元素是另一个元素的子元素。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "<div> <!-- 这是父元素 -->\n",
    "    <p>这是子元素</p> <!-- 这是子元素 -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，`<div>` 元素是 `<p>` 元素的父元素，而 `<p>` 元素是 `<div>` 元素的子元素。子元素通常位于父元素的内部，并且可以通过嵌套来创建更复杂的结构。\n",
    "\n",
    "- 兄弟元素关系： 兄弟元素关系指的是位于同一父元素内的元素之间的关系，它们具有相同的父元素。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "<ul> <!-- 父元素 -->\n",
    "    <li>Item 1</li> <!-- 这是兄弟元素 -->\n",
    "    <li>Item 2</li> <!-- 这是兄弟元素 -->\n",
    "    <li>Item 3</li> <!-- 这是兄弟元素 -->\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，`<li>` 元素之间就是兄弟元素，它们都是 `<ul>` 元素的子元素，并且具有相同的父元素。兄弟元素通常位于同一层次结构中，它们可以通过 CSS 选择器和 JavaScript 来选择和操作。\n",
    "\n",
    "让我们再回过头来看看标签 `<body>` 的子元素：\n",
    "\n",
    "- `<h1>` 标签定义一个一级标题，用于显示重要的文本。\n",
    "- `<p>` 标签定义段落，用于包含普通文本。\n",
    "- `<ul>` 和 `<li>` 标签定义无序列表，包含链接列表项。\n",
    "- `<a>` 标签定义链接，使用 href 属性指定链接的目标 URL。\n",
    "\n",
    "尝试将上面的示例代码输入到记事本中保存，并将文件后缀修改为 .HTML，这样我们就得到了一个带有相应内容的网页，将其在浏览器中打开，观察其中的元素与代码之间的联系。你还可以尝试修改一下代码的内容，自定义你的第一个网页。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
