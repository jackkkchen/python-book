{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"爬虫基础\"\n",
    "format:\n",
    "  html:\n",
    "   code-fold: false\n",
    "   code-tools: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫基础\n",
    "\n",
    "## 爬虫的基本概念\n",
    "\n",
    "爬虫，即一段自动抓取互联网信息的程序，从互联网上抓取对于我们有价值的信息。\n",
    "\n",
    "根据爬虫的定义，很容易联想到各类浏览器、搜索引擎，我们日常生活中使用浏览器搜索网站，也可以看作是在使用爬虫爬取信息。只不过，浏览器的搜索结果往往是一整个网页，而我们常常只需要网页中特定的内容。比如，当我们想要获取当前电影市场上评分最高的 250 部电影，我们可以使用浏览器进行搜索。、\n",
    "\n",
    "爬虫，即抓取互联网信息的自动化程序。借助爬虫，我们可以从互联网上抓取对于我们有价值的信息。\n",
    "\n",
    "我们很容易从爬虫的定义联想到各类浏览器，似乎我们日常中使用浏览器所进行的搜索活动，与爬虫的工作差不多。这样的想法并没有错，浏览器搜索网站的过程，也可以看作是在使用爬虫获取信息。只不过，浏览器往往会将所有的网页信息都获取下来，而我们常常只需要网页中的特定内容。比如，假设我需要获取当前豆瓣电影网站上，评分最高的 250 部电影的信息，使用浏览器进行搜索时，结果常常是这样的：\n",
    "\n",
    "![豆瓣电影_TOP250](img\\豆瓣电影_TOP250.png)\n",
    "\n",
    "仔细观察搜索结果，我们会发现，在这个页面中，除了我们想要的电影信息，还有许多不相干的元素，比如页面右边的各种口碑榜、票房榜等等，而我们只需要电影的相关信息。因此，我们可以尝试使用爬虫来从该页面中获取到我们想要的电影信息（电影的名称、排行、评分、评价人数、影评），并且整理成为如下图所示的表格。\n",
    "\n",
    "![爬取结果](img\\爬取结果.png)\n",
    "\n",
    "### 一、爬虫的概念\n",
    "\n",
    "Python 爬虫从本质上来说，就是一种自动化程序，被设计用来在互联网上获取特定的信息。它模仿人类在网页上进行浏览的行为，可以在短时间内处理大量的网页和数据。Python 爬虫通常可以用于从网页中抓取数据、分析信息、搜索特定内容等场景。\n",
    "\n",
    "根据前面的介绍，我们已经知道了，Python 爬虫就是一段自动化代码，通过各种条件判断、循环等操作来从互联网上抓取我们想要的内容。爬虫是如何模拟人类的操作的呢？要知道这一点，我们就首先要了解在日常中，我们使用浏览器进行搜索时，是如何与服务器建立连接的。另外，由于服务器返回给客户端的内容往往不是整理好的表格或文本，而是 HTML 文档，因此，我们也需要了解 HTML 文档的构成，以帮助我们从中提取出我们想要的部分。\n",
    "\n",
    "这样一来，我们似乎整理出了爬虫程序的基本运行方式，即：\n",
    "\n",
    "1. 请求库(HTTP 请求)：首先，爬虫使用 HTTP 请求库（如 `requests`）向目标网站发送请求，获取网页的 HTML 源代码。在这个步骤中，我们获取了需要被处理的网页内容。\n",
    "2. 网页解析：接下来，我们需要解析获取到的 HTML 源代码以获取有用的信息。在这一步中，我们使用 HTML 解析库（如`BeautifulSoup` 等）或正则表达式等方法来完成。\n",
    "3. 数据提取：在解析的基础上，爬虫可以使用解析库提供的工具（如标签解析、`XPath`、`CSS` 选择器）来从 HTML 源代码中提取所需的数据。这些数据可以是网页的标题、段落、链接、图像等等。\n",
    "4. 数据存储：最后，提取到的数据可以被存储在本地文件、数据库或其他的数据存储方式中，以备后续的分析、使用或展示。这一步骤使得爬虫获得的有用信息可以被进一步处理和利用\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "\n",
    "id1(发送 HTTP 请求) --> id2(网页解析) --> id3(数据提取) --> id4(数据存储)\n",
    "```\n",
    "\n",
    "根据爬虫的基本运作方式，我们也就了解了学习爬虫的一般步骤。首先，我们要了解什么是 HTTP 请求，什么是 HTML 文档。随后，我们就可以开始学习如何使用 HTTP 请求库来获取网页的基本数据。最后，我们再掌握解析网页数据，并从中提取关键信息的方法。另外，为了更高效地获取数据，简化爬虫的开发过程，我们还将了解主流的爬虫框架，学习其构成及使用方法。\n",
    "\n",
    "现在，让我们来看看本次学习的基本规划吧：\n",
    "\n",
    "首先，我们将会学习 HTTP&HTTPS 与 HTML 的概念，并且了解如何从一个网页中找到我们所需要的信息。\n",
    "\n",
    "接下来，我们学习 `requests` 库，这是一个简洁而功能强大的Python库，用于发送HTTP请求和处理HTTP响应。它使得在Python中进行网络请求变得更加简单，提供了一种直观的方式来与Web服务进行交互。\n",
    "\n",
    "随后，我们学习 `BeautifulSoup` 库，使用这个流行的解析库来解析我们在上一步中所获取到的数据。\n",
    "\n",
    "最后，我们学习 `scrapy` 框架，它是一个高级的、开源的Python爬虫框架，专门设计用于快速、高效地从网站中提取结构化数据。它提供了一整套工具和抽象层，使得开发者能够轻松地创建、管理和执行各种类型的网络爬虫任务。\n",
    "\n",
    "### 二、爬虫策略与爬虫伦理\n",
    "\n",
    "在正式开始爬虫的学习之前，我们还有一个十分重要的任务要做，就是了解爬虫策略与爬虫伦理。\n",
    "\n",
    "首先我们要了解，爬虫作为一种技术手段，本身是不违法任何法律条文的，但是开发者使用爬虫进行网络数据获取的行为可能会涉及到法律风险。这是因为，虽然许多网站向用户免费提供服务，放开了数据获取权限，但是网站的数据依然是受到版权或知识产权保护的，随意使用爬虫不仅会触犯到网站所有者的权益，也可能危害到数据隐私和个人信息。另外，网站的运营和维护都是需要成本的，网站的服务器资源也是有限的，如果不受限制地使用爬虫，会加重网站的运行负担，占用过多的服务器资源，甚至无异于 DDoS 攻击，这会导致网站服务器过载从而无法正常提供服务，也会影响到其他用户的使用。\n",
    "\n",
    "使用爬虫的法律风险包括但不限于：\n",
    "\n",
    "- 侵犯版权和知识产权： 爬虫可能会获取并使用受版权保护的内容，如文本、图片、音频等。在未经授权的情况下使用这些内容可能构成版权侵犯，导致法律纠纷。\n",
    "- 数据隐私和个人信息： 爬虫获取的数据可能包含个人隐私信息，如姓名、邮箱、电话号码等。在处理这些信息时，需要遵守相关数据隐私法规，避免侵犯他人隐私权。\n",
    "- 反竞争行为： 如果使用爬虫获取竞争对手的商业信息，可能被视为不正当竞争行为，可能触发法律纠纷。\n",
    "- 网络犯罪： 如果爬虫被用于非法活动，如网络钓鱼、诈骗等，可能构成网络犯罪，导致刑事责任。\n",
    "- 违反网站条款： 有些网站在其服务条款中规定了爬虫行为的限制，如果不遵守这些规定，可能导致法律风险。\n",
    "- 分布式拒绝服务攻击（DDoS）： 如果爬虫请求过于频繁，可能会对网站服务器造成压力，被视为DDoS攻击，导致法律追究。\n",
    "- 机器人协议（robots.txt）： 忽略网站的robots.txt文件，即使不是法律违规，也可能导致网站封禁你的爬虫，影响数据获取。\n",
    "- 法律地域差异： 不同国家和地区对爬虫活动的法律规定不同，需要根据实际情况遵循当地法律法规。\n",
    "\n",
    "基于以上因素，在学习爬虫之前，我们首先要了解爬虫策略与爬虫伦理。\n",
    "\n",
    "#### 1. 爬虫策略\n",
    "\n",
    "爬虫策略是指开发者在进行网络爬虫任务时制定的计划和方法。它涉及爬取的目标、深度、频率、数据提取、并发等方面的决策，以确保爬虫能够高效、有序地从目标网站抓取数据。合理的爬虫策略可以降低对目标网站的负担，减少被封禁的风险，同时也能够保证数据的准确性和完整性。\n",
    "\n",
    "爬虫策略的一些重要考虑因素包括：\n",
    "\n",
    "- 爬取目标： 确定要爬取的网站或页面，明确所需的数据。\n",
    "- 爬取深度： 决定爬虫深入网站的层级，即爬取多少个链接的链接。\n",
    "- 爬取频率： 设定请求频率，避免对网站造成过大的压力，防止被封禁。\n",
    "- 数据提取： 确定如何从网页中提取所需的数据，使用选择器、正则表达式等方法。\n",
    "- 请求头设置： 设置适当的User-Agent、Referer等请求头，模拟正常用户行为。\n",
    "- 并发处理： 考虑如何有效地处理多个并发请求，提高爬取效率。\n",
    "- 错误处理： 处理网络错误、连接超时等问题，设置重试机制。\n",
    "\n",
    "设置合理的爬虫策略，不仅有利于进行更高效更准确的数据爬取，也有利于保护自身不触犯法律风险。\n",
    "\n",
    "#### 2. 爬虫伦理\n",
    "\n",
    "为了设置合理的爬虫策略，我们就需要了解爬虫伦理。\n",
    "\n",
    "爬虫伦理是指在进行网络爬虫任务时应遵循的道德和法律规范。因为爬虫可能会对目标网站产生一定的影响，开发者应该遵循以下几个伦理原则：\n",
    "\n",
    "- 尊重网站规则： 开发者应该遵循目标网站的 robots.txt 文件，避免爬取被禁止的页面。\n",
    "- 避免过度请求： 爬虫不应该对目标网站造成不必要的负担，应该合理设置请求频率和间隔。\n",
    "- 尊重隐私： 爬虫不应该获取或传播个人隐私信息，如邮箱、密码等。\n",
    "- 遵循法律法规： 爬虫开发者应该遵守国家和地区的相关法律法规，避免侵犯他人权益。\n",
    "- 避免滥用： 爬虫不应该被用于恶意目的，如大规模数据盗取、DDoS攻击等。\n",
    "- 保护目标网站： 爬虫应该避免对目标网站造成影响，如不要频繁爬取大量页面。\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "**robots.txt 文件**\n",
    "\n",
    "robots.txt 文件是一个用于指导网络爬虫如何爬取网站内容的标准文件。这个文件通常位于网站的根目录下，我们可以通过在网站域名后面添加 /robots.txt 来访问。\n",
    "\n",
    "robots.txt 文件的主要作用是告诉网络爬虫哪些页面可以爬取，哪些页面不应该被爬取。它是网站所有者与搜索引擎和其他爬虫之间的一种协议，有助于管理网络爬虫的行为，以确保爬虫不会访问或爬取敏感、私人或不希望被索引的内容。\n",
    "\n",
    "我们可以打开任意一个网站，在其域名后添加  来访问其 robots.txt 文件。下面是一个简单的 robots.txt 文件示例：\n",
    "\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /private/\n",
    "Disallow: /admin/\n",
    "Allow: /public/\n",
    "Crawl-delay: 10\n",
    "```\n",
    "\n",
    "这个文件的结构为：\n",
    "\n",
    "User-agent：这个部分指定了要针对哪个爬虫或用户代理设置规则。通常使用一个星号 * 来表示适用于所有爬虫，或者指定爬虫的名称或代号。\n",
    "Disallow：这个部分规定了爬虫不允许访问的页面或目录。例如，Disallow: /private/ 表示不允许爬虫访问以 /private/ 开头的所有页面。\n",
    "Allow：与Disallow相对应，这个部分规定了允许访问的页面或目录。通常不是所有的爬虫都支持这个指令。\n",
    "Crawl-delay：这个部分指定了爬虫爬取网站页面的延迟时间，以秒为单位。例如，Crawl-delay: 5 表示爬虫每隔5秒才能请求一个页面。\n",
    "\n",
    ":::\n",
    "\n",
    "总之，爬虫策略和伦理是网络爬虫开发中不可忽视的重要方面。通过合理的策略和遵循伦理原则，开发者可以实现有效的数据抓取，同时避免对网站和其他用户造成不必要的困扰和损害。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## HTTP 协议\n",
    "\n",
    "### 一、HTTP 的概念\n",
    "\n",
    "首先，让我们一起来了解一下，什么是 HTTP 协议。\n",
    "\n",
    "在前面对爬虫的介绍中，我们讲到，爬虫程序会模拟人类的行为，向浏览器发送请求，与服务器建立连接。在这一步中，与服务器建立连接的行为，就使用到了 HTTP 协议。\n",
    "\n",
    "HTTP 即 Hypertext Transfer Protocol(超文本传输协议)，是一种用于在**客户端**与**服务器**之间的传输数据的**请求 - 响应协议**。在我们的日常生活中，最常用到 HTTP 协议的地方就是在浏览器中浏览网页，我们在浏览器的地址栏中输入一个网址，浏览器就会向该网站的服务器发送一个请求。在这一过程中，浏览器与服务器之间遵循的传输协议，通常就是 HTTP 协议。\n",
    "\n",
    "HTTP 是 Web 应用程序和互联网上的服务器之间通信的基础，它被设计用于传输超文本(通常指 HTML 文档)，但是也可以传输其他类型的数据，如图像、音视频文件等。\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "id1(客户端) -- HTTP 协议 --> id2(服务器)\n",
    "```\n",
    "\n",
    "> HTTPS 即 Hypertext Transfer Protocol Secure，是基于 HTTP 提出的一种**安全**超文本传输协议，即 HTTP 的安全版本。HTTPS 旨在保护网络通信的隐私和安全性，它通过多种加密方式来加密数据传输，防止敏感信息在传输过程中被恶意窃听或篡改。\n",
    ">\n",
    "> 关于 HTTP & HTTPS 有以下几点需要注意：\n",
    ">\n",
    "> 关于 HTTPS，需要注意的点是：\n",
    ">\n",
    "> - 端口：默认情况下，HTTPS 使用 TCP 端口443，而 HTTP 使用端口80。\n",
    "> - URL前缀：HTTPS 网站的 URL 以 \"https://\" 开头，而 HTTP 网站以 \"http://\" 开头。这个前缀是浏览器的一种提示，告诉用户连接是安全的。\n",
    "> - 身份验证：HTTPS 可以通过数字证书对服务器进行身份验证，确保客户端正在连接到合法的服务器，而不是中间人攻击者。数字证书由可信的证书颁发机构（CA）颁发。\n",
    "> - 数据完整性：HTTPS 还提供了数据完整性检查，以确保在传输过程中数据没有被篡改或损坏。数据完整性是通过使用消息摘要算法（如 SHA-256）和数字签名来实现的。\n",
    "\n",
    "### 二、URI 的概念\n",
    "\n",
    "在学习 HTTP 时，我们还需要了解一些相关的知识——即 URI 与 URL。\n",
    "\n",
    "URI(Uniform Resource Identifier)的中文名称叫做“统一资源标识符”，顾名思义，它的作用就是标识和定位资源(不仅仅是互联网上的资源)。URI 分为两种形式：URL 和 URN。\n",
    "\n",
    "#### 1. URL(Uniform Resource Locator)\n",
    "\n",
    "URL 是 URI 的一个子集，它就是我们常说的“统一资源定位符”，它包含了资源的地址以及获取该资源所需要的协议信息。URL 通常以特定的协议(如 HTTP、HTTPS、FTP 等）开头，后面跟着资源的地址和可能的其他信息。\n",
    "\n",
    "一个完整的 URL 通常由三部分构成：比如 https://www.example.com/index.html 是一个URL，它包括了协议(HTTPS)、主机名(www.example.com)以及资源路径(/index.html)。\n",
    "\n",
    "任意打开一个网页，仔细观察它的地址栏。\n",
    "\n",
    "![URL](img\\URL.png)\n",
    "\n",
    "这里有一串长长的字符，就是我们常说的网址，即 URL。现在，让我们将其拆分为几块来看，分别为：\n",
    "\n",
    "- 传输协议：`https://`\n",
    "- 域名：`pydatacamp.com`\n",
    "- 资源路径：`/05_01_爬虫基础.html`\n",
    "\n",
    "掌握 URL 的知识，对我们后面学习使用爬虫获取资源十分有必要。\n",
    "\n",
    "#### 2. URN(Uniform Resource Name)\n",
    "\n",
    "URN 是 URI 的另一种子集，又叫做“统一资源名称”，它用于标识资源的名称，而不涉及资源的位置或如何获取资源。URN 的一个例子是 ISBN(国际标准书号)，它标识了一本书的唯一名称，而不管它存储在哪里。\n",
    "\n",
    "### 三、HTTP 的请求与响应\n",
    "\n",
    "#### 1. HTTP 请求\n",
    "\n",
    "客户端要使用 HTTP 协议向服务器发送请求，就要遵循 HTTP 的请求方法。HTTP 协议支持多种请求方法以实现获取数据、创建数据、删除数据等功能，其中最常见的两种方法就是 `GET` 和 `POST`。\n",
    "\n",
    "- `GET` 方法主要用于获取数据，比如当我们浏览某一个网页时，浏览器就会像服务器发送 `GET` 请求，用以获取页面数据。\n",
    "\n",
    "- `POST` 方法主要用于创建数据，比如当我们注册账号时，浏览器就会向服务器发送 `POST` 请求，将用户数据传递给服务器。\n",
    "\n",
    "本次学习我们主要讲解爬虫的基本概念，大多情况下只需要获取浏览器返回的数据，因此我们主要学习 `GET` 方法。\n",
    "\n",
    "一个 HTTP 请求通常由**请求行**、**请求头**、**请求体**三个部分组成，其中，请求体常常用来向服务器传输数据，因此 `GET` 方法，通常没有请求体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "GET /top250 HTTP/1.1<!-- 请求行(Request Line) -->\n",
    "Host: movie.douban.com<!-- 请求头(Request Headers) -->\n",
    "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\n",
    "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\n",
    "Accept-Language: en-US,en;q=0.9\n",
    "Connection: keep-alive\n",
    "<!-- 空行 -->\n",
    "<!-- 请求体(Request Body)-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，HTTP 请求的结构如下：\n",
    "\n",
    "- 请求行(Request Line)\n",
    "  - 请求方法：`GET`\n",
    "  - 资源路径：`/top250`\n",
    "  - 协议版本：`HTTP/1.1`\n",
    "- 请求头(Request Headers)\n",
    "  - 主机名(Host)：`movie.douban.com`\n",
    "  - User-Agent：`Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36`\n",
    "  - 可以接受的响应内容类型(Accept)：`text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9`\n",
    "  - 语言首选项(Accept-Language)：`en-US,en;q=0.9`\n",
    "  - 连接状态(Connection)：`keep-alive`\n",
    "- 空行(Blank Line)：空行用于分隔请求头部和请求体。\n",
    "- 请求体(Request Body)：`GET` 请求通常没有请求体，因此此处为空。\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "id1(HTTP 请求) --> id2(请求行)\n",
    "id1 --> id3(请求头)\n",
    "id1 --> id4(请求体)\n",
    "```\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "在上面的示例中，还有一些特殊的点需要注意\n",
    "\n",
    "1. 请求行中的资源路径指明了客户端想要访问的服务器中的资源，在资源路径后面，还可以填入一个 `?`，用以添加查询参数，用以向服务器传递额外的信息，不同的信息之间用 `&` 分隔。如：\n",
    "\n",
    "```html\n",
    "www.douban.com/movie/top250?start=750&filter=unwatched\n",
    "```\n",
    "\n",
    "2. 请求头会包含一些传递给服务器的信息，其中：\n",
    "   (1) `Host` 指主机域名，主机域名结合请求行里面的资源路径，就可以得到一个完整的网址\n",
    "     (2) `User-Agent` 用来告知服务器客户端的相关信息，比如请求发起者的身份等等\n",
    "     (3) `Accept` 告诉服务器客户端希望接受的响应数据的类型，多种类型以 `,` 分隔， `*/*` 表示接受任意类型的数据\n",
    "\n",
    "3. 请求体一般包含客户端想要传递给服务器的其他任意数据，`GET` 方法的请求体通常为空。\n",
    "\n",
    ":::\n",
    "\n",
    "#### 2. HTTP 响应\n",
    "\n",
    "当服务器接收到客户端发出的 HTTP 请求后，会根据请求返回 HTTP 响应。类似的，HTTP 响应也由三个部分组成：状态行、响应头、响应体。观察下面的例子，这是一个完整的 HTTP 响应。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "HTTP/1.1 200 OK<!-- 状态行(Status Line) -->\n",
    "Date: Sat, 28 Aug 2023 12:00:00 GMT<!-- 响应头(Response Headers) -->\n",
    "Server: Apache/2.4.41 (Unix)\n",
    "Content-Type: text/html; charset=UTF-8\n",
    "Content-Length: 1234\n",
    "Connection: keep-alive\n",
    "\n",
    "<!DOCTYPE html><!-- 响应体(Response Body) -->\n",
    "<html>\n",
    "\n",
    "<head>\n",
    "    <title>Welcome to Example.com</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <h1>Hello, World!</h1>\n",
    "    <p>This is an example HTML page.</p>\n",
    "</body>\n",
    "\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个 HTTP 响应的结构如下：\n",
    "\n",
    "- 状态行(Status Line)\n",
    "  - 协议版本：`HTTP/1.1`\n",
    "  - 响应状态码：`200 OK`\n",
    "- 响应头(Response Headers)\n",
    "  - 时间日期(Date)：`Sat, 28 Aug 2023 12:00:00 GMT`\n",
    "  - 服务器软件信息(Server)：`Apache/2.4.41 (Unix)`\n",
    "  - 响应内容类型(Content-Type)：`text/html; charset=UTF-8`\n",
    "  - 响应体长度(Content-Length)：`1234`\n",
    "  - 连接状态(Connection)：`keep-alive`\n",
    "- 空行(Blank Line)：空行用于分隔响应头和响应体。\n",
    "- 响应体(Response Body)：响应体包含了服务器返回的 HTML 内容，即页面的结构和内容。\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "id1(HTTP 响应) --> id2(状态行)\n",
    "id1 --> id3(响应头)\n",
    "id1 --> id4(响应体)\n",
    "```\n",
    "\n",
    "关于服务器返回的 HTTP 响应，需要格外注意的是状态行中的**响应状态码**，它代表了服务器对客户端的请求的响应状态。状态码共分为五类：\n",
    "\n",
    "1. `1xx`(信息性状态码)：这些状态码是信息性的，表示请求已被接收，继续处理。最常见的 `1xx` 状态码是 `100 Continue`，表示服务器已经接收到请求的一部分，客户端可以继续发送请求的其余部分。\n",
    "2. `2xx`(成功状态码)：这些状态码表示请求已成功接收、理解和接受。最常见的 `2xx` 状态码是 `200 OK`，表示请求成功。\n",
    "3. `3xx`(重定向状态码)：这些状态码表示客户端需要采取额外的操作来完成请求。通常用于指示资源已经移动到新的URL，需要重新发起请求。常见的 `3xx` 状态码包括 `301 Moved Permanently`(永久重定向)和`302 Found`(临时重定向)。\n",
    "4. `4xx`(客户端错误状态码)：这些状态码表示客户端发送的请求有错误或无法完成。最常见的 `4xx` 状态码是 `400 Bad Request`(请求无效)和 `403 Forbidden`(禁止访问)。这些状态码通常表示客户端需要修改请求以使其有效。\n",
    "5. `5xx`(服务器错误状态码)：这些状态码表示服务器在尝试处理请求时发生了错误。最常见的 `5xx` 状态码是 `500 Internal Server Error`（服务器内部错误)。这些状态码通常表示服务器出现问题，无法处理请求。\n",
    "\n",
    "下面的表格中给出了常见的 HTTP 状态码及其含义：\n",
    "\n",
    "| 状态码 | 含义 |\n",
    "| :----: | :--: |\n",
    "| 200 OK | 客户端请求成功 |\n",
    "| 201 Moved Permanently | 资源被永久移动到新地址 |\n",
    "| 400 Bad Request | 客户端不能被服务器所理解 |\n",
    "| 401 Unauthorized | 请求未经授权 |\n",
    "| 403 Forbidden | 服务器拒绝提供服务 |\n",
    "| 404 Not Found | 请求资源不存在 |\n",
    "| 500 Internal Server Error | 服务器发生不可预期的错误 |\n",
    "| 503 Service Unavailable | 服务器当前不能处理客户端的请求 |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
