{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"爬虫基础\"\n",
    "format:\n",
    "  html:\n",
    "   code-fold: false\n",
    "   code-tools: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫基础\n",
    "\n",
    "## 爬虫的基本概念\n",
    "\n",
    "爬虫，即一段自动抓取互联网信息的程序，从互联网上抓取对于我们有价值的信息。\n",
    "\n",
    "根据爬虫的定义，很容易联想到各类浏览器、搜索引擎，我们日常生活中使用浏览器搜索网站，也可以看作是在使用爬虫爬取信息。只不过，浏览器的搜索结果往往是一整个网页，而我们常常只需要网页中特定的内容。比如，当我们想要获取当前电影市场上评分最高的 250 部电影，我们可以使用浏览器进行搜索。、\n",
    "\n",
    "爬虫，即抓取互联网信息的自动化程序。借助爬虫，我们可以从互联网上抓取对于我们有价值的信息。\n",
    "\n",
    "我们很容易从爬虫的定义联想到各类浏览器，似乎我们日常中使用浏览器所进行的搜索活动，与爬虫的工作差不多。这样的想法并没有错，浏览器搜索网站的过程，也可以看作是在使用爬虫获取信息。只不过，浏览器往往会将所有的网页信息都获取下来，而我们常常只需要网页中的特定内容。比如，假设我需要获取当前豆瓣电影网站上，评分最高的 250 部电影的信息，使用浏览器进行搜索时，结果常常是这样的：\n",
    "\n",
    "![豆瓣电影_TOP250](./img/豆瓣电影_TOP250.png)\n",
    "\n",
    "仔细观察搜索结果，我们会发现，在这个页面中，除了我们想要的电影信息，还有许多不相干的元素，比如页面右边的各种口碑榜、票房榜等等，而我们只需要电影的相关信息。因此，我们可以尝试使用爬虫来从该页面中获取到我们想要的电影信息（电影的名称、排行、评分、评价人数、影评），并且整理成为如下图所示的表格。\n",
    "\n",
    "![爬取结果](./img/爬取结果.png)\n",
    "\n",
    "### 一、爬虫的概念\n",
    "\n",
    "Python 爬虫从本质上来说，就是一种自动化程序，被设计用来在互联网上获取特定的信息。它模仿人类在网页上进行浏览的行为，可以在短时间内处理大量的网页和数据。Python 爬虫通常可以用于从网页中抓取数据、分析信息、搜索特定内容等场景。\n",
    "\n",
    "这样一来，我们似乎整理出了爬虫程序的基本运行方式，即：\n",
    "\n",
    "1. 请求库(HTTP 请求)：首先，爬虫使用 HTTP 请求库 `requests` 获取网页的 HTML 源代码。\n",
    "2. 网页解析：接下来，使用 HTML 解析库 `BeautifulSoup` 对获取到的网页源代码进行解析。\n",
    "3. 数据提取：使用标签解析、`XPath`、`CSS` 选择器来从 HTML 源代码中提取所需的数据。\n",
    "4. 数据存储：最后，将提取到的数据存储在本地文件、数据库或其他的数据存储方式中。\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "\n",
    "id1(发送 HTTP 请求) --> id2(网页解析) --> id3(数据提取) --> id4(数据存储)\n",
    "```\n",
    "\n",
    "根据爬虫的基本运作方式，我们也就了解了学习爬虫的一般步骤。首先，我们要了解什么是 HTTP 请求，什么是 HTML 文档。随后，我们就可以开始学习如何使用 HTTP 请求库来获取网页的基本数据。最后，我们再掌握解析网页数据，并从中提取关键信息的方法。另外，为了更高效地获取数据，简化爬虫的开发过程，我们还将了解主流的爬虫框架，学习其构成及使用方法。\n",
    "\n",
    "\n",
    "### 二、爬虫伦理\n",
    "\n",
    "为了在使用爬虫时不触犯法律风险，我们就需要了解爬虫伦理。\n",
    "\n",
    "爬虫伦理是指在进行网络爬虫任务时应遵循的道德和法律规范。因为爬虫可能会对目标网站产生一定的影响，开发者应该遵循以下几个伦理原则：\n",
    "\n",
    "- 尊重网站规则\n",
    "- 避免过度请求\n",
    "- 尊重隐私\n",
    "- 遵循法律法规\n",
    "- 避免滥用\n",
    "- 保护目标网站\n",
    "\n",
    "**robots.txt 文件**\n",
    "\n",
    "robots.txt 文件是一个用于指导网络爬虫如何爬取以及是否允许爬取网站内容的标准文件，这个文件通常位于网站的根目录下。\n",
    "\n",
    "我们可以打开任意一个网站的根目录，在其域名后添加 `/robots.txt` 来访问。\n",
    "\n",
    "下面是一个简单的 robots.txt 文件示例：\n",
    "\n",
    "![](./img/robots.png)\n",
    "\n",
    "这个文件的结构为：\n",
    "\n",
    "User-agent：这个部分指定了要针对哪个爬虫或用户代理设置规则。通常使用一个星号 * 来表示适用于所有爬虫，或者指定爬虫的名称或代号。\n",
    "\n",
    "Disallow：这个部分规定了爬虫不允许访问的页面或目录。例如，Disallow: /private/ 表示不允许爬虫访问以 /private/ 开头的所有页面。\n",
    "\n",
    "Allow：与Disallow相对应，这个部分规定了允许访问的页面或目录。通常不是所有的爬虫都支持这个指令。\n",
    "\n",
    "Crawl-delay：这个部分指定了爬虫爬取网站页面的延迟时间，以秒为单位。例如，Crawl-delay: 5 表示爬虫每隔5秒才能请求一个页面。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 链接解析\n",
    "\n",
    "URL，也就是我们常说的网址，它的正式名称是“统一资源定位符”，它是互联网上的路标，指向了我们所需要的资源。因此，要学习爬虫，就要了解什么是 URL。我们把一个网址分成四个部分来说明，以 <https://movie.douban.com/top250?filter=unwatched> 为例，分别是：\n",
    "\n",
    "- 安全传输协议：`https://`\n",
    "- 主域名：`movie.douban.com`\n",
    "- 子链接：`/top250`\n",
    "- 筛选规则：`?filter=unwatched`\n",
    "\n",
    "### 一、传输协议\n",
    "\n",
    "#### 1. HTTP 的概念\n",
    "\n",
    "首先，让我们一起来了解一下，什么是 HTTP 协议。\n",
    "\n",
    "在前面对爬虫的介绍中，我们讲到，爬虫程序会模拟人类的行为，向浏览器发送请求，与服务器建立连接。在这一步中，与服务器建立连接的行为，就使用到了 HTTP 协议。\n",
    "\n",
    "HTTP 即 Hypertext Transfer Protocol(超文本传输协议)，是一种用于在**客户端**与**服务器**之间的传输数据的**请求 - 响应协议**。比如，我们在浏览网页时，浏览器与服务器之间遵循的传输协议，通常就是 HTTP 协议。\n",
    "\n",
    "HTTP 通常会显示不安全，例如 <http://www.moe.gov.cn/>，如果网站受到攻击，密码容易泄露。\n",
    "\n",
    "![](./img/https.png)\n",
    "\n",
    "随着互联网的不断发展，HTTP 协议正在逐渐被淘汰，HTTPS 协议已成为越来越多网站的标配，为用户提供更为安全和可靠的网络环境。\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "id1(客户端) -- HTTP 协议 --> id2(服务器)\n",
    "```\n",
    "\n",
    "<!-- HTTP 是 Web 应用程序和互联网上的服务器之间通信的基础，它被设计用于传输超文本(通常指 HTML 文档)，但是也可以传输其他类型的数据，如图像、音视频文件等。\n",
    "\n",
    "> HTTPS 即 Hypertext Transfer Protocol Secure，是基于 HTTP 提出的一种**安全**超文本传输协议，即 HTTP 的安全版本。HTTPS 旨在保护网络通信的隐私和安全性，它通过多种加密方式来加密数据传输，防止敏感信息在传输过程中被恶意窃听或篡改。\n",
    "\n",
    "### 二、URI 的概念\n",
    "\n",
    "在学习 HTTP 时，我们还需要了解一些相关的知识——即 URI 与 URL。\n",
    "\n",
    "URI(Uniform Resource Identifier)的中文名称叫做“统一资源标识符”，顾名思义，它的作用就是标识和定位资源(不仅仅是互联网上的资源)。URI 分为两种形式：URL 和 URN。\n",
    "\n",
    "#### 1. URL(Uniform Resource Locator)\n",
    "\n",
    "URL 是 URI 的一个子集，它就是我们常说的“统一资源定位符”，它包含了资源的地址以及获取该资源所需要的协议信息。URL 通常以特定的协议(如 HTTP、HTTPS、FTP 等）开头，后面跟着资源的地址和可能的其他信息。\n",
    "\n",
    "一个完整的 URL 通常由三部分构成：比如 https://www.example.com/index.html 是一个URL，它包括了协议(HTTPS)、主机名(www.example.com)以及资源路径(/index.html)。\n",
    "\n",
    "任意打开一个网页，仔细观察它的地址栏。\n",
    "\n",
    "![URL](./img/URL.png)\n",
    "\n",
    "这里有一串长长的字符，就是我们常说的网址，即 URL。现在，让我们将其拆分为几块来看，分别为：\n",
    "\n",
    "- 传输协议：`https://`\n",
    "- 域名：`pydatacamp.com`\n",
    "- 资源路径：`/05_01_爬虫基础.html`\n",
    "\n",
    "掌握 URL 的知识，对我们后面学习使用爬虫获取资源十分有必要。\n",
    "\n",
    "#### 2. URN(Uniform Resource Name)\n",
    "\n",
    "URN 是 URI 的另一种子集，又叫做“统一资源名称”，它用于标识资源的名称，而不涉及资源的位置或如何获取资源。URN 的一个例子是 ISBN(国际标准书号)，它标识了一本书的唯一名称，而不管它存储在哪里。 -->\n",
    "\n",
    "#### 2. HTTP 的请求与响应\n",
    "\n",
    "##### (1) HTTP 请求\n",
    "\n",
    "客户端要使用 HTTP 协议向服务器发送请求，就要遵循 HTTP 的请求方法。HTTP 协议支持多种请求方法以实现获取数据、创建数据、删除数据等功能。在爬虫中，最常用的就是 `GET` 方法。\n",
    "\n",
    "`GET` 方法主要用于获取数据，比如当我们浏览某一个网页时，浏览器就会像服务器发送 `GET` 请求，用以获取页面数据。\n",
    "\n",
    "一个 HTTP 请求通常由**请求行**、**请求头**、**请求体**三个部分组成，其中，请求体常常用来向服务器传输数据，因此 `GET` 方法，通常没有请求体。下面是一个 `GET` 请求的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "GET /top250 HTTP/1.1<!-- 请求行(Request Line) -->\n",
    "Host: movie.douban.com<!-- 请求头(Request Headers) -->\n",
    "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\n",
    "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\n",
    "Accept-Language: en-US,en;q=0.9\n",
    "Connection: keep-alive\n",
    "<!-- 空行 -->\n",
    "<!-- 请求体(Request Body)-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，HTTP 请求的结构如下：\n",
    "\n",
    "- 请求行(Request Line)\n",
    "  - 请求方法：`GET`\n",
    "  - 资源路径：`/top250`\n",
    "  - 协议版本：`HTTP/1.1`\n",
    "- 请求头(Request Headers)\n",
    "  - 主机名(Host)：`movie.douban.com`\n",
    "  - User-Agent：`Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36`\n",
    "  - 可以接受的响应内容类型(Accept)：`text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9`\n",
    "  - 语言首选项(Accept-Language)：`en-US,en;q=0.9`\n",
    "  - 连接状态(Connection)：`keep-alive`\n",
    "- 空行(Blank Line)：空行用于分隔请求头部和请求体。\n",
    "- 请求体(Request Body)：`GET` 请求通常没有请求体，因此此处为空。\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "id1(HTTP 请求) --> id2(请求行)\n",
    "id1 --> id3(请求头)\n",
    "id1 --> id4(请求体)\n",
    "```\n",
    "\n",
    "<!-- ::: {.callout-note}\n",
    "\n",
    "在上面的示例中，还有一些特殊的点需要注意\n",
    "\n",
    "1. 请求行中的资源路径指明了客户端想要访问的服务器中的资源，在资源路径后面，还可以填入一个 `?`，用以添加查询参数，用以向服务器传递额外的信息，不同的信息之间用 `&` 分隔。如：\n",
    "\n",
    "```html\n",
    "www.douban.com/movie/top250?start=750&filter=unwatched\n",
    "```\n",
    "\n",
    "2. 请求头会包含一些传递给服务器的信息，其中：\n",
    "   (1) `Host` 指主机域名，主机域名结合请求行里面的资源路径，就可以得到一个完整的网址\n",
    "     (2) `User-Agent` 用来告知服务器客户端的相关信息，比如请求发起者的身份等等\n",
    "     (3) `Accept` 告诉服务器客户端希望接受的响应数据的类型，多种类型以 `,` 分隔， `*/*` 表示接受任意类型的数据\n",
    "\n",
    "3. 请求体一般包含客户端想要传递给服务器的其他任意数据，`GET` 方法的请求体通常为空。\n",
    "\n",
    "::: -->\n",
    "\n",
    "##### (2) HTTP 响应\n",
    "\n",
    "当服务器接收到客户端发出的 HTTP 请求后，会根据请求返回 HTTP 响应。类似的，HTTP 响应也由三个部分组成：状态行、响应头、响应体。观察下面的例子，这是一个完整的 HTTP 响应。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "HTTP/1.1 200 OK<!-- 状态行(Status Line) -->\n",
    "Date: Sat, 28 Aug 2023 12:00:00 GMT<!-- 响应头(Response Headers) -->\n",
    "Server: Apache/2.4.41 (Unix)\n",
    "Content-Type: text/html; charset=UTF-8\n",
    "Content-Length: 1234\n",
    "Connection: keep-alive\n",
    "\n",
    "<!DOCTYPE html><!-- 响应体(Response Body) -->\n",
    "<html>\n",
    "\n",
    "<head>\n",
    "    <title>Welcome to Example.com</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <h1>Hello, World!</h1>\n",
    "    <p>This is an example HTML page.</p>\n",
    "</body>\n",
    "\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个 HTTP 响应的结构如下：\n",
    "\n",
    "- 状态行(Status Line)\n",
    "  - 协议版本：`HTTP/1.1`\n",
    "  - 响应状态码：`200 OK`\n",
    "- 响应头(Response Headers)\n",
    "  - 时间日期(Date)：`Sat, 28 Aug 2023 12:00:00 GMT`\n",
    "  - 服务器软件信息(Server)：`Apache/2.4.41 (Unix)`\n",
    "  - 响应内容类型(Content-Type)：`text/html; charset=UTF-8`\n",
    "  - 响应体长度(Content-Length)：`1234`\n",
    "  - 连接状态(Connection)：`keep-alive`\n",
    "- 空行(Blank Line)：空行用于分隔响应头和响应体。\n",
    "- 响应体(Response Body)：响应体包含了服务器返回的 HTML 内容，即页面的结构和内容。\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "id1(HTTP 响应) --> id2(状态行)\n",
    "id1 --> id3(响应头)\n",
    "id1 --> id4(响应体)\n",
    "```\n",
    "\n",
    "关于服务器返回的 HTTP 响应，需要格外注意的是状态行中的**响应状态码**，它代表了服务器对客户端的请求的响应状态。状态码共分为五类：\n",
    "\n",
    "1. `1xx`(信息性状态码)\n",
    "2. `2xx`(成功状态码)\n",
    "3. `3xx`(重定向状态码)\n",
    "4. `4xx`(客户端错误状态码)\n",
    "5. `5xx`(服务器错误状态码)\n",
    "\n",
    "下面的表格中给出了常见的 HTTP 状态码及其含义：\n",
    "\n",
    "| 状态码 | 含义 |\n",
    "| :----: | :--: |\n",
    "| 200 OK | 客户端请求成功 |\n",
    "| 201 Moved Permanently | 资源被永久移动到新地址 |\n",
    "| 400 Bad Request | 客户端不能被服务器所理解 |\n",
    "| 401 Unauthorized | 请求未经授权 |\n",
    "| 403 Forbidden | 服务器拒绝提供服务 |\n",
    "| 404 Not Found | 请求资源不存在 |\n",
    "| 500 Internal Server Error | 服务器发生不可预期的错误 |\n",
    "| 503 Service Unavailable | 服务器当前不能处理客户端的请求 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、域名与资源\n",
    "\n",
    "前面我们已经了解到，URL 就是指向互联网资源的路标，给浏览器一个 URL 就可以定位到网络上的一个资源。刚才我们解析了网址 <https://movie.douban.com/top250?filter=unwatched> 开头的传输协议部分 `https`，现在，让我们一起来看看后面的路径部分。\n",
    "\n",
    "就像我们电脑上的文件地址一样，URL 的功能就是在网络上对一个资源进行定位。你可以将一个网址拆分成几部分，并且向理解电脑上的文件地址一样理解它。\n",
    "\n",
    "对于网址 <https://movie.douban.com/top250?filter=unwatched>，我们可以这样来看，比如在你的 Windows 电脑中，你的用户文件夹下，有一个名为 \"MyDocuments\" 的文件夹，这个文件夹下有一个名为 \"report.docx\" 的文件，那么它的路径就是：\"C:\\Users\\YourUsername\\MyDocuments\\report.docx\"。\n",
    "\n",
    "现在让我们试着类比：\n",
    "\n",
    "- 协议：在 URL 最前面的协议部分，可以理解为我们电脑上文件的**“打开方式”**，只有用文件支持的打开方式才能打开它，同理，只有遵循网址的通讯协议，才能与其建立连接。\n",
    "- 主域名：`www.douban.com` 这部分就是域名，它类似于文件的文件路径的主文件夹，就是 \"C:\\Users\" 部分。\n",
    "- 子域名：`www.movie.douban.com` 就是 URL 的子域名，它就像主文件夹下面的子文件夹，就是 \"\\YourUsername\\MyDocuments\" 这一部分。\n",
    "- 资源路径：`/top250` 就是网络上的资源路径，它指向了网站 `www.movie.douban.com` 下的一个榜单：电影 TOP250，就像文件的具体位置一样 \"\\report.docx\"。\n",
    "- 查询参数：`?filter=unwatched` 就是文件的查询参数，它用于筛选特定的内容，就像电脑上文件的附加信息一样。\n",
    "\n",
    "让我们一起跟着这个比喻，来看看域名与资源的关系。\n",
    "\n",
    "#### 1. 域名\n",
    "\n",
    "现在，让我们以 `www.movie.douban.com` 为例来解释域名的结构。\n",
    "\n",
    "和我们日常生活中的阅读习惯不同，域名的阅读是从右到左进行的。其结构为：\n",
    "\n",
    "- 顶级域名：`.com` \n",
    "- 二级域名：`.douban`\n",
    "- 二级域名的子部分：`.movie`\n",
    "- 子域名：`www`\n",
    "\n",
    "这个网址共有以上四个组成部分。让我们一个个来看：\n",
    "\n",
    "##### (1) 顶级域名\n",
    "\n",
    "顶级域名 `.com` 是一个通用顶级域名，通常与商业网站相关。我们在《流浪地球 2》中看到的根服务器，就是用来存放和维护顶级域名的。顶级域名是互联网中最高层次的域名，一般位于 URL 的最右侧。顶级域名通常由国际组织、国家或地区的注册机构进行管理，并根据其用途分为不同的类别。以下是一些常见的顶级域名：\n",
    "\n",
    "- 通用顶级域名\n",
    "  - `.com`：通用商业顶级域名，用于商业网站。\n",
    "  - `.org`：通用组织顶级域名，用于非盈利组织。\n",
    "  - `.net`：通用网络顶级域名，用于与互联网和网络服务相关的机构。\n",
    "  - `.gov`：通用政府顶级域名，用于政府机构。\n",
    "  - `.edu`：通用教育顶级域名，用于教育机构\n",
    "- 国家和地区代码顶级域名\n",
    "  - `.cn`：中国的顶级域名\n",
    "  - `.us`：美国的顶级域名\n",
    "\n",
    "了解一些常见的顶级域名，可以让我们在看到一个 URL 时就大概知道其用途。\n",
    "\n",
    "##### (2) 二级域名\n",
    "\n",
    "在上面的示例中，`.douban` 就是一个二级域名。二级域名通常表示特定的组织、公司、网站或服务。例如，`.douban` 就由豆瓣公司掌握，`.zhihu` 就由中文知识社区知乎掌握。\n",
    "\n",
    "##### (3) 二级域名的子部分\n",
    "\n",
    "这一部分比较特殊，在上面的 URL 中，我们可以看到，除了常见的顶级域名 `.com`，二级域名 `.douban`，子域名`www`，还有一个部分，就是 `.movie`。`.movie` 在这里是 `.douban.com` 域名的一个子域名，代表了豆瓣网站中的电影板块。这样的作法可以使网站的功能划分更为明确。但是要注意，如果 `.movie` 在 URL 的最右侧，那么它代表的就是一个通用顶级域名，表示电影相关的网站。\n",
    "\n",
    "##### (4) 子域名\n",
    "\n",
    "子域名就是 URL 最左侧的部分 `www`，它表示世界范围内的万维网服务，是访问网站的标准方式。\n",
    "\n",
    "将上面的四个部分合并起来，就是主域名：`www.movie.douban.com`。这个 URL 告诉我们：\n",
    "\n",
    "- 它是一个位于 `.com` 顶级域名下的域名；\n",
    "- 它的名称是 `douban`，归属于一个特定的公司；\n",
    "- 它提供的服务是 `movie`，表示电影板块；\n",
    "- 它的访问方式是 `www`，属于万维网的一部分。\n",
    "\n",
    "#### 2. 查询参数\n",
    "\n",
    "网站的查询参数是用于在 URL 中传递额外信息或数据的一种方式，通常位于 UTL 的末尾，以便向服务器发送请求时提供特定的参数或选项。上面示例中的 `?filter=unwatched`，就是查询参数，表示未观看过的电影。\n",
    "\n",
    "要知道网站支持哪些查询参数，通常只有两种方法，一种是得到网站官方的支持，比如阅读其官方文档或联系网站支持；另一种是总结归纳，通过自己或其他开发者的归纳，来获取网站支持的查询参数。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
