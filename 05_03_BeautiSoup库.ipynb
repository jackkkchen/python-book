{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"BeautifulSoup 库\"\n",
    "format:\n",
    "  html:\n",
    "   code-fold: false\n",
    "   code-tools: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  BeautifulSoup 库\n",
    "\n",
    "### 一、BeautifulSoup 库的安装与使用\n",
    "\n",
    "BeautifulSoup 库是一个 Python 库，用于从 HTML 和 XML 文档中提取数据，解析文档结构，并使其易于浏览和操作。BeautifulSoup 库的主要目标是使数据抽取和处理变得简单、可靠，并且支持 Python 标准库中的多种解析器。在本次学习中，我们将使用 BeautifulSoup 库来解析上一节中，使用 Requests 库获取到的数据。\n",
    "\n",
    "> [BeautifulSoup 库的官方文档 ](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)：<https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/> \n",
    ">\n",
    "> 🤔BeautifulSoup 库作为世界上最流行的 Python 爬虫解析库之一，它有中文名字吗？直接翻译过来应该叫做“美丽汤”库，不过我们或许可以叫它“美汁汁”库！😆😆🥳🥳🙌🙌🎉🎊\n",
    "\n",
    "作为一个第三方库，BeautifulSoup 库和其他所有库一样，在使用前需要进行安装，输入下面的代码，然后重启你的内核："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup 库一共支持四种解析器，下面的表格详细展示了各个解析器的优缺点以及使用方法；\n",
    "\n",
    "| 解析器           | 使用方法                                                     | 优势                                                         | 劣势                                              |\n",
    "| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------- |\n",
    "| Python 标准库    | `BeautifulSoup(待处理对象, \"html.parser\")`                   | - Python的内置标准库<br />- 执行速度适中<br />- 文档容错能力强 | - Python 2.7.3 or 3.2.2)前 的版本中文档容错能力差 |\n",
    "| lxml HTML 解析器 | `BeautifulSoup(markup, \"lxml\")`                              | - 速度快<br />- 文档容错能力强                               | - 需要安装C语言库                                 |\n",
    "| lxml XML 解析器  | `BeautifulSoup(markup, [\"lxml-xml\"])`<br />**or**<br />`BeautifulSoup(markup, \"xml\")` | - 速度快<br />- 唯一支持XML的解析器                          | - 需要安装C语言库                                 |\n",
    "| html5lib         | `BeautifulSoup(markup, \"html5lib\")`                          | - 最好的容错性<br />- 以浏览器的方式解析文档<br />- 生成HTML5格式的文档 | - 速度慢<br />- 不依赖外部扩展                    |\n",
    "\n",
    "看着中间两项解析器，你有没有一点儿熟悉？没错，这就是我们在学习 XPath 时了解过的 lxml 库！在后面的学习中，我们大多使用这个解析器。仔细看它的劣势：需要安装 C 语言库，即使我们曾经安装过，但是为了以防万一，还是保险一点儿好。尝试下面的安装代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你已经做好了前置工作，试试看下面的代码，看看能否正常工作：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--STATUS OK--><html> <head><meta content=\"text/html;charset=utf-8\" http-equiv=\"content-type\"/><meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/><meta content=\"always\" name=\"referrer\"/><link href=\"https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css\" rel=\"stylesheet\" type=\"text/css\"/><title>百度一下，你就知道</title></head> <body link=\"#0000cc\"> <div id=\"wrapper\"> <div id=\"head\"> <div class=\"head_wrapper\"> <div class=\"s_form\"> <div class=\"s_form_wrapper\"> <div id=\"lg\"> <img height=\"129\" hidefocus=\"true\" src=\"//www.baidu.com/img/bd_logo1.png\" width=\"270\"/> </div> <form action=\"//www.baidu.com/s\" class=\"fm\" id=\"form\" name=\"f\"> <input name=\"bdorz_come\" type=\"hidden\" value=\"1\"/> <input name=\"ie\" type=\"hidden\" value=\"utf-8\"/> <input name=\"f\" type=\"hidden\" value=\"8\"/> <input name=\"rsv_bp\" type=\"hidden\" value=\"1\"/> <input name=\"rsv_idx\" type=\"hidden\" value=\"1\"/> <input name=\"tn\" type=\"hidden\" value=\"baidu\"/><span class=\"bg s_ipt_wr\"><input autocomplete=\"off\" autofocus=\"autofocus\" class=\"s_ipt\" id=\"kw\" maxlength=\"255\" name=\"wd\" value=\"\"/></span><span class=\"bg s_btn_wr\"><input autofocus=\"\" class=\"bg s_btn\" id=\"su\" type=\"submit\" value=\"百度一下\"/></span> </form> </div> </div> <div id=\"u1\"> <a class=\"mnav\" href=\"http://news.baidu.com\" name=\"tj_trnews\">新闻</a> <a class=\"mnav\" href=\"https://www.hao123.com\" name=\"tj_trhao123\">hao123</a> <a class=\"mnav\" href=\"http://map.baidu.com\" name=\"tj_trmap\">地图</a> <a class=\"mnav\" href=\"http://v.baidu.com\" name=\"tj_trvideo\">视频</a> <a class=\"mnav\" href=\"http://tieba.baidu.com\" name=\"tj_trtieba\">贴吧</a> <noscript> <a class=\"lb\" href=\"http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1\" name=\"tj_login\">登录</a> </noscript> <script>document.write('<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ '\" name=\"tj_login\" class=\"lb\">登录</a>');\n",
      "                </script> <a class=\"bri\" href=\"//www.baidu.com/more/\" name=\"tj_briicon\" style=\"display: block;\">更多产品</a> </div> </div> </div> <div id=\"ftCon\"> <div id=\"ftConw\"> <p id=\"lh\"> <a href=\"http://home.baidu.com\">关于百度</a> <a href=\"http://ir.baidu.com\">About Baidu</a> </p> <p id=\"cp\">©2017 Baidu <a href=\"http://www.baidu.com/duty/\">使用百度前必读</a>  <a class=\"cp-feedback\" href=\"http://jianyi.baidu.com/\">意见反馈</a> 京ICP证030173号  <img src=\"//www.baidu.com/img/gs.gif\"/> </p> </div> </div> </div> </body> </html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "html = requests.get('https://www.baidu.com')\n",
    "html.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(html.text,'lxml')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上面的代码，你将获得由 BeautifulSoup 解析好的 HTML 文档。\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "小提示：\n",
    "\n",
    "如果你觉得直接输出的 HTML 过于大块而且难看，可以使用 `.prettify()` 方法进行输出，就和这个方法的名字一样，会让你的输出变得漂亮起来！🦋🦋\n",
    "\n",
    "将上面的输出代码替换成：\n",
    "\n",
    "```python \n",
    "print(soup.prettify())\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "使用 BeautifulSoup 库解析 HTML 文档看上去是不是十分简单？就像表格里面列出来的那样，你只需要将文件传递给 BeautifulSoup，然后指定解析器，它就会自动开始工作了！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(待处理对象, \"解析器\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、BeautifulSoup 库的对象类型\n",
    "\n",
    "BeautifulSoup 库解析复杂 HTML 文档的方式是将 HTML 文档转换为一个复杂的树形结构，每个节点都是 Python 对象。在 BeautifulSoup 中，对象一共可以归纳为 4 种。\n",
    "\n",
    "#### 1. `Tag` 标签\n",
    "\n",
    "`Tag` 对象，就是 HTML 原生文档中的标签，每个 `Tag` 对象都代表文档中的一个具体标签。对于 `Tag` 对象，我们主要学习其最重要的两个属性：\n",
    "\n",
    "##### (1) `name` 标签名\n",
    "\n",
    "`name` 属性是 `Tag` 属性中最重要的属性之一，该属性用于表示标签的名称，允许永固访问和修改标签的名称。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签名称: div\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<div class=\"container\">\n",
    "    <p>这是一个段落。</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "div_tag = soup.find('div')\n",
    "\n",
    "# 访问标签的名称\n",
    "print(\"标签名称:\", div_tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在上面的示例中，我们定义了一个简短的 HTML 字符串，然后通过 BeautifulSoup 中的 lxml 解析器来解析它。随后，我们通过 `.find()` 方法找到了标签 `<div>`，最后，使用 `name` 属性将其的标签名成功输出出来了。\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "**`.find()` 方法**\n",
    "\n",
    "`.find()` 方法是 BeautifulSoup 库中最常用的查找**文档中第一个匹配指定条件的标签**的方法。其语法为：\n",
    "\n",
    "```python \n",
    "find(name, attrs, recursive, text)\n",
    "```\n",
    "\n",
    "参数解释：\n",
    "\n",
    "- `name`：要查找的标签的名称或标签名列表。可以是字符串，正则表达式，函数，或它们的组合。例如，`'a'` 表示查找 `<a>` 标签，`['a', 'div']` 表示查找 `<a>` 或 `<div>` 标签。\n",
    "- `attrs`：可选参数，用于指定要匹配的标签的属性字典。例如，`{'class': 'example', 'id': 'content'}` 表示查找具有特定类和 ID 属性的标签。\n",
    "- `recursive`：可选参数，控制是否在标签的所有子孙层次中查找。默认为 True，表示查找所有子孙标签，如果设置为 False，只会在直接子标签中查找。\n",
    "- `text`：可选参数，用于查找标签内的文本内容。可以是字符串，正则表达式，函数，或它们的组合。\n",
    "\n",
    "`.find()` 方法的返回值是**第一个**满足条件的标签对象，如果找不到匹配的标签，则返回 `None`。\n",
    "\n",
    "```python \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <p>第一段.</p>\n",
    "    <div class=\"example\">这是一个 div.</div>\n",
    "    <a href=\"https://www.example.com\">点这里</a>\n",
    "    <a href=\"https://www.example_2.com\">咔哒咔哒</a>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# 查找第一个<p>标签\n",
    "paragraph = soup.find('p')\n",
    "print(paragraph)\n",
    "```\n",
    "\n",
    "```python \n",
    "# 查找具有'class'属性为'example'的第一个<div>标签\n",
    "div_with_class = soup.find('div', class_='example')\n",
    "print(div_with_class)\n",
    "```\n",
    "\n",
    "```python \n",
    "# 查找第一个包含指定文本的标签\n",
    "link = soup.find(string='点这里')\n",
    "print(link)\n",
    "```\n",
    "\n",
    "```python \n",
    "# 查找具有属性'href'的第一个<a>标签\n",
    "anchor_with_href = soup.find('a', href=True)\n",
    "print(anchor_with_href)\n",
    "```\n",
    "\n",
    "另外，你还可以使用 `.find_all` 方法获取所有的符合条件的标签\n",
    "\n",
    "```python \n",
    "# 查找所有的<a>标签，返回列表\n",
    "all_links = soup.find_all('a')\n",
    "print(all_links)\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "通过访问标签名，我们可以找到我们所需要的数据结构，以支持我们对 HTML 文档进行数据提取、条件筛选等操作。\n",
    "\n",
    "BeautifulSoup 库还支持修改标签名，当我们需要重构 HTML 文档或者进行数据清洗时，就可以使用 `replace_with()` 方法来更改标签名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<section class=\"container\">\n",
      "<p>这是一个段落。</p>\n",
      "</section>\n"
     ]
    }
   ],
   "source": [
    "# 更改标签名为 'section'\n",
    "div_tag.name = 'section'\n",
    "\n",
    "# 打印修改后的标签\n",
    "print(div_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将这段代码添加到上一段示例代码之后，运行它，你会发现，我们定义的 HTML 字符串中的 `<div>` 标签被替换为了 `<section>` 标签。这样一来，我们就通过访问和变更 `name` 属性，对文档的结构进行了调整。\n",
    "\n",
    "在变更标签的 `name` 属性时，需要非常谨慎的操作，以确保不破坏文档的结构和语义。\n",
    "\n",
    "##### (2) `Attributes` 属性\n",
    "\n",
    "在 HTML 文档中，`Attributes` 是与标签关联的额外信息，通常以“键-值”对的形式出现，其中“键”表示属性的名称，“值”表示属性的值。\n",
    "\n",
    "> 看到“键-值”对，你有没有想起来什么？🤔没错！就是字典！属性在 BeautifulSoup 中通常以字典的形式表示，`Tag` 的属性的操作方法与字典相同。这意味着，`Tag` 的属性可以被添加、修改和删除！\n",
    "\n",
    "要访问某个 `Tag` 的属性，我们可以使用 `.attrs` 方法。如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'href': 'https://www.example.com', 'class': ['link']}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<a href=\"https://www.example.com\" class=\"link\">Click here</a>\n",
    "<img src=\"image.jpg\" alt=\"An example image\">\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "\n",
    "# 获取 <a> 标签的属性字典\n",
    "a_tag = soup.find('a')\n",
    "a_attrs = a_tag.attrs\n",
    "\n",
    "# 输出 <a> 标签的属性\n",
    "print(a_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的示例中，我们在解析完 HTML 字符串以后，首先使用 `.find()` 方法定位，找到了 `<a>` 标签，随后，使用 `.attrs` 方法获取了 `<a>` 标签的属性列表。\n",
    "\n",
    "如果你想要指定某个属性进行获取，可以直接在指定的标签后使用 `[]` 来获取该属性，语法为：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag['属性']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继续使用上面的例子来获取指定的属性：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "链接地址: https://www.example.com\n",
      "类名: ['link']\n",
      "图片源: image.jpg\n",
      "替代文本: An example image\n"
     ]
    }
   ],
   "source": [
    "# 获取<a>标签的属性字典\n",
    "a_tag = soup.find('a')\n",
    "a_attrs = a_tag.attrs\n",
    "\n",
    "# 访问属性\n",
    "print(\"链接地址:\", a_attrs['href'])\n",
    "print(\"类名:\", a_attrs['class'])\n",
    "\n",
    "# 获取<img>标签的属性字典\n",
    "img_tag = soup.find('img')\n",
    "img_attrs = img_tag.attrs\n",
    "\n",
    "# 访问属性\n",
    "print(\"图片源:\", img_attrs['src'])\n",
    "print(\"替代文本:\", img_attrs['alt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，我们使用 `a_attrs` 和 `img_attrs` 分别获取了标签 `<a>` 和 `<img>` 的属性字典，然后，我们使用字典的方式访问这些标签的属性，例如，使用 `a_attrs['href']` 来获取链接地址。字典形式的属性让我们能够轻松地检索和操作标签的属性信息，便于数据的提取和文档处理。\n",
    "\n",
    "另外，你还可以使用 `.get()` 方法来获取指定的属性：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag.get('属性')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是我们并不推荐这种方式，使用字典的方式来操作标签的属性，既直观又容易记忆，你只需要记住，`tag` 的属性操作方法与字典一样就可以了。\n",
    "\n",
    "#### 2. `NavigableString` 可以遍历的字符串\n",
    "\n",
    "`NavigableString` 顾名思义，就是可以遍历的字符串。在 HTML 文档中，字符串通常包含在 `Tag` 内，因此，`NavigableString` 用于表示标签内的文本内容。当 BeautifulSoup 解析 HTML 或 XML 文档时，它会将标签内的文本作为 `NavigableString` 对象来表示，从而可以访问和操作这些文本内容。\n",
    "\n",
    "`NavigableString` 字符串与 Python 中的字符串的属性相同，你可以对 `NavigableString` 对象的文本内容进行我们在 Python 基础中学过的那些字符串操作，包括字符串的方法、切片、查找、替换等等。你甚至可以直接使用 `unicode()` 方法将 `NavigableString` 转换成 Python 中的 `Unicode` 字符串。\n",
    "\n",
    "使用 `.string` 方法获取 `NavigableString` 的文本内容，就像下面这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本内容: 这是一段话吱吱吱\n",
      "对象类型: <class 'bs4.element.NavigableString'>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<p>这是一段话吱吱吱</p>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "\n",
    "# 获取<p>标签内的文本内容，返回一个NavigableString对象\n",
    "p_tag = soup.find('p')\n",
    "text_inside_p = p_tag.string\n",
    "\n",
    "# 打印文本内容\n",
    "print(\"文本内容:\", text_inside_p)\n",
    "print(\"对象类型:\", type(text_inside_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，我们获取了 `<p>` 标签内的文本内容，并且将其输出了出来，同时还检查了其类型，为：`<class 'bs4.element.NavigableString'>`。\n",
    "\n",
    "需要注意的是，`Tag` 中的字符串不能直接编辑，但可以被替换为其他字符串。要实现这一操作，我们可以使用 `replace_with()` 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本内容: 这是一段话吱吱吱\n"
     ]
    }
   ],
   "source": [
    "# 替换文本内容\n",
    "p_tag.replace_with(\"这是一段话喵喵喵\")\n",
    "text_inside_p = p_tag.string\n",
    "\n",
    "# 打印文本内容\n",
    "print(\"文本内容:\", text_inside_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. `Comment` 注释及特殊字符串\n",
    "\n",
    "在 BeautifulSoup 中，注释是一个特殊的类型，使用 `Comment` 表示，代表 HTML 文档中的注释内容。`Comment` 对象通常被包裹在 `<!--` 和 `-->` 标记之间，只可以读取，不可以修改。\n",
    "\n",
    "`Comment` 对象的特殊点在于，它和 `NavigableString` 一样，也可以通过 `.string` 属性访问，访问的结果将返回注释的文本内容。但是在进行爬虫操作时，我们常常不需要爬取注释内容，因此，在使用爬虫时，要格外注意 HTML 文档中的注释内容。下面的例子有些难以理解，不要着急，仔细阅读它。\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "要判断一个对象是不是 `Comment` 类型，我们可以使用 `isinstance(待判断对象，判断格式)` 方法。这个函数结构两个参数，一个是待判断的对象，另外一个是我们想要判断的格式。举例来说，就像下面这样：\n",
    "\n",
    "```python \n",
    "isinstance(my_comment, Comment)\n",
    "```\n",
    "\n",
    "这个例子使用 `isinstance()` 方法，来检查 `my_comment` 对象是不是 `Comment` 类型。\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注释内容:  这是一个注释。 \n",
      "对象类型: <class 'bs4.element.Comment'>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <body>\n",
    "    <p>这是一个段落。</p>\n",
    "    <!-- 这是一个注释。 -->\n",
    "    <div>\n",
    "        <p>这是另外一个锻炼。</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# 查找所有注释并打印其内容\n",
    "comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "for comment in comments:\n",
    "    print(\"注释内容:\", comment.string)\n",
    "    print(\"对象类型:\", type(comment.string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，我们首先使用 `find_all` 方法，想要用它来找到所有符合我们要求的对象；然后，我们在括号内传入了一个表达式 `string=lambda text: isinstance(text, Comment)`，其中，`string` 参数用于指定我们要查找的内容，匿名函数 `lambda text: isinstance(text, Comment)` 用来对我们要查找的内容做出条件判断，对每个文本内容进行检查，看它是否是一个 `Comment` 对象；最后，我们使用了一个循环，将所有查询到的 `Comment` 对象都输出出来了。\n",
    "\n",
    "#### 4. `BeautifulSoup`\n",
    "\n",
    "最后一个对象，看起来是不是很奇怪？🤔😆你没有眼花！最后一个对象就是 `BeautifulSoup` 对象。它是 BeautifSoup 库中的主要对象，表示一个文档的全部内容。它通常用于解析和操作 HTML 或 XML 文档。`BeautifulSoup` 对象提供了许多方法和属性，使我们能够轻松地遍历文档、查找标签、提取数据和修改文档的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[document]\n",
      "{}\n",
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html>\n",
    "  <head>\n",
    "    <title>举一个例子🌰</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p>这是一个段落</p>\n",
    "    <a href=\"https://www.example.com\">点击！点击！🖱️</a>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# 创建BeautifulSoup对象并指定解析器\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "\n",
    "# 查看BeautifuiSoup对象的属性\n",
    "print(soup.name)\n",
    "print(soup.attrs)\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会发现 `BeautifulSoup` 对象的属性列表为空，这是因为 `BeautifulSoup` 对象并不是一个真正的 `Tag`，它没有常规的 `name` 和 `attribute` 属性。上面的示例中所查看到的 `name` 属性是 `BeautifulSoup` 对象的一个特殊属性。\n",
    "\n",
    "### 三、遍历文档树\n",
    "\n",
    "#### 1. 文档树基础\n",
    "\n",
    "BeautifulSoup 中的文档树是一个抽象的树状结构，用于表示 HTML 文档的层次结构和关系。文档树包含标签、文本内容、注释、特殊字符等各种元素，以及它们之间的嵌套关系。\n",
    "\n",
    "对于文档树，我们要了解的部分主要有：\n",
    "\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "id(文档树) --> id1(组成元素)\n",
    "id --> id2(元素之间的关系)\n",
    "id1 --> id3(根元素)\n",
    "id1 --> id4(标签)\n",
    "id1 --> id5(文本内容)\n",
    "id1 --> id6(属性)\n",
    "id1 --> id7(注释)\n",
    "id2 --> id10(父子关系)\n",
    "id2 --> id11(兄弟关系)\n",
    "id2 --> id12(前后节点)\n",
    "```\n",
    "\n",
    "文档树的组成元素，我们已经在上面进行了一定的了解，接下来，让我们一起来看看元素之间关系，了解文档树的构成 。\n",
    "\n",
    "#### 2. 元素间的关系\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "我们已经在上面了解过，操作文档树最简单的方式就是使用 `Tag` 的名字，通过直接使用标签的名字，就可以获取标签的内容，只需要在 `BeautifulSoup` 对象后跟上 `Tag` 的名字就可以了。\n",
    "\n",
    "```python \n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>简短的HTML示例</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>欢迎来到我的网站</h1>\n",
    "    <p>这是一个简短的HTML示例。</p>\n",
    "    <ul>\n",
    "        <li>项目1</li>\n",
    "        <li>项目2</li>\n",
    "        <li>项目3</li>\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "```\n",
    "\n",
    "这个方法是可以多次使用的，例如：\n",
    "\n",
    "```python \n",
    "print(soup.body.h1.p)\n",
    "```\n",
    "\n",
    "但需要注意的是，点取属性的方式只能获得指定标签名的第一个 `Tag`，如果想要获得所有的指定标签名的 `Tag`，可以使用 `find_all()` 方法。\n",
    "\n",
    "```python \n",
    "print(soup.find_all('li'))\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "##### (1) 父子关系\n",
    "\n",
    "在文档树中，父子关系节点是最常见的节点关系。BeautifulSoup 提供了许多方法来对父子关系节点进行操作。\n",
    "\n",
    "1. 获取子节点\n",
    "\n",
    "   想要获取某一个 `Tag` 的子节点，可以使用 `.contents` 属性。`.contents` 属性可以将 `Tag` 的子节点以列表的方式输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', <title>父子关系示例</title>, '\\n']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>父子关系示例</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"parent\">\n",
    "        <h1>这是一个标题</h1>\n",
    "        <p>这是一个段落。</p>\n",
    "        <ul>\n",
    "            <li>列表项1</li>\n",
    "            <li>列表项2</li>\n",
    "            <li>列表项3</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "\n",
    "# 使用 contents 方式获取子节点\n",
    "head_tag = soup.head\n",
    "head_tag_contents = head_tag.contents\n",
    "\n",
    "# 输出获取结果\n",
    "print(head_tag_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   需要注意的是，由于字符串没有子节点，因此字符串不可以使用 `.contents` 属性。幸运的是，我们还有另外一种方法：`.children`。这个方法返回的是子节点的迭代器，需要用循环来输出其中的内容。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "父子关系示例\n"
     ]
    }
   ],
   "source": [
    "title_tag =soup.title\n",
    "for child in title_tag.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.contents` 和 `.children` 属性仅包含 `tag` 的直接子节点，如果想要将一个 `Tag` 的全部子节点都输出出来，可以使用 `.descendants` 属性。`.descendants` 可以获取到指定标签下的所有子孙标签，但是该方法也是返回的子孙节点的迭代器，因此也要使用循环来i遍历所有的子孙标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1\n",
      "p\n",
      "ul\n",
      "li\n",
      "li\n",
      "li\n"
     ]
    }
   ],
   "source": [
    "# 找到起始标签\n",
    "div_tag = soup.find('div')\n",
    "\n",
    "# 使用 .descendants 遍历所有子孙标签\n",
    "for tag in div_tag.descendants:\n",
    "    if tag.name is not None:  # 检查标签是否有名字，避免处理字符串\n",
    "        print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 获取父节点\n",
    "\n",
    "与子节点的获取相对应，BeautifulSoup 也提供了获取父节点的方法。我们可以通过 `.parent` 属性来获取某个元素的父节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head>\n",
      "<title>父子关系示例</title>\n",
      "</head>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>父子关系示例</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"parent\">\n",
    "        <h1>这是一个标题</h1>\n",
    "        <p>这是一个段落。</p>\n",
    "        <ul>\n",
    "            <li>列表项1</li>\n",
    "            <li>列表项2</li>\n",
    "            <li>列表项3</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "\n",
    "# 使用 parent 方式获取父节点\n",
    "title_tag = soup.title\n",
    "print(title_tag.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与子节点不同的是，字符串也有父节点，就是它本身：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>父子关系示例</title>\n"
     ]
    }
   ],
   "source": [
    "print(title_tag.string.parent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档的顶层节点的父节点是 `BeautifulSoup` 对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "html_tag = soup.html\n",
    "print(type(html_tag.parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` 对象的父节点是 `None`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(soup.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外的，使用 `.parents` 属性可以递归得到元素的所有父辈节点，但与属性类似，想要使用循环才能遍历完某标签的所有父辈节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'parents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\CloudDrive\\OneDrive\\3_Library\\Code\\Pandas_Book\\python-book\\05_03_BeautiSoup库.ipynb 单元格 45\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CloudDrive/OneDrive/3_Library/Code/Pandas_Book/python-book/05_03_BeautiSoup%E5%BA%93.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m li \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mli\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CloudDrive/OneDrive/3_Library/Code/Pandas_Book/python-book/05_03_BeautiSoup%E5%BA%93.ipynb#Y104sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m parent \u001b[39min\u001b[39;00m li\u001b[39m.\u001b[39;49mparents:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CloudDrive/OneDrive/3_Library/Code/Pandas_Book/python-book/05_03_BeautiSoup%E5%BA%93.ipynb#Y104sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m parent \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CloudDrive/OneDrive/3_Library/Code/Pandas_Book/python-book/05_03_BeautiSoup%E5%BA%93.ipynb#Y104sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mprint\u001b[39m(parent)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'parents'"
     ]
    }
   ],
   "source": [
    "li = soup.li\n",
    "\n",
    "for parent in li.parents:\n",
    "    if parent is None:\n",
    "        print(parent)\n",
    "    else:\n",
    "        print(parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) 兄弟关系\n",
    "\n",
    "拥有同一个父节点，且处于同一缩进级别的节点就是兄弟节点。使用 `.next_sibling` 来查询下一个兄弟节点，`.previous_sibling` 来查询上一个兄弟节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个<p>标签的下一个兄弟节点是： <a>这是第二个段落。</a>\n",
      "标签<a>的上一个兄弟节点是： <p>这是第一个段落。</p>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>兄弟标签示例</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>兄弟标签示例</h1>\n",
    "    <p>这是第一个段落。</p><a>这是第二个段落。</a>\n",
    "    <p>这是第三个段落。</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "\n",
    "# 使用.next_sibling获取下一个兄弟节点\n",
    "sibling_soup = soup.p.next_sibling\n",
    "print(\"第一个<p>标签的下一个兄弟节点是：\", sibling_soup)\n",
    "\n",
    "# 使用.previous_sibling获取上一个兄弟节点\n",
    "sibling_soup_2 = soup.a.previous_sibling\n",
    "print(\"标签<a>的上一个兄弟节点是：\", sibling_soup_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码很清晰地展现出了各个节点之间的关系，但是如果你尝试修改了一下上面的代码，想要查询第二个 `<p>` 与标签 `<a>` 之间的关系，可能会失望。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a>标签的下一个兄弟节点是： \n",
      "\n",
      "第二个标签<p>的上一个兄弟节点是： \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用.next_sibling获取下一个兄弟节点\n",
    "sibling_soup = soup.a.next_sibling\n",
    "print(\"<a>标签的下一个兄弟节点是：\", sibling_soup)\n",
    "\n",
    "# 使用.previous_sibling获取上一个兄弟节点\n",
    "sibling_soup_2 = soup.p.previous_sibling\n",
    "print(\"第二个标签<p>的上一个兄弟节点是：\", sibling_soup_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细观察运行结果，我们发现，代码既没有报错，也没有预想中的输出。这是因为，空格和换行符都是标签的兄弟节点。因此，实际 HTML 文档中的 `Tag` 的 `.next_sibling` 和 `.previous_sibling` 属性通常都是字符串或空白。\n",
    "\n",
    "##### (3) 前后节点\n",
    "\n",
    "现在我们来看看最后一种元素间的关系：前后节点。\n",
    "\n",
    "在运行 HTML 解析器时，你有没有想过它是如何工作的？看看下面的代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于上面这一小块 HTML 文档，解析器的工作流程是：\n",
    "\n",
    "```{mermaid}\n",
    "graph LR\n",
    "id(打开 &lthtml&gt 标签) --> id1(打开一个 &lthead&gt 标签) --> id2(打开一个 &lttitle&gt 标签) -->  id3(添加一段字符串) --> id4(关闭 &lttitle&gt 标签) --> id5(打开 &ltp&gt 标签) -->id6(...)\n",
    "```\n",
    "\n",
    "根据解析器的工作流程，我们就可以定义出前后节点关系：在指定节点后解析的节点叫做该节点的前进节点，在指定节点前解析的节点叫做回退节点。我们可以使用 `.next_element` 属性指向解析过程中下一个被解析的对象(字符串或 `tag`)，即指向前进节点；使用 `.previous_element` 属性指向当前被解析的对象的前一个解析对象，即指向回退节点。\n",
    "\n",
    "有时候前后节点可能与兄弟节点是一样的，但通常是不一样的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
