{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Scrapy 框架\"\n",
    "format:\n",
    "  html:\n",
    "   code-fold: false\n",
    "   code-tools: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy 框架\n",
    "\n",
    "在前面的学习中，我们使用 requests 库抓取到了我们想要的网站的全部源代码，但通常情况下，我们并不需要整个网站的代码，而只需要我们所关注的部分的数据即可。因此，我们需要有一个新的工具，来帮助我们对网页中的内容进行分析，并提取我们所需要的内容。\n",
    "\n",
    "scrapy 是一个强大的 Python 框架，具有高度模块化的设计，包括各种组件，通过这些组件的协同工作，用户可以实现高效的网络爬取。\n",
    "\n",
    "::: {.callout-note}\n",
    "**爬虫框架**\n",
    "\n",
    "所谓爬虫框架就是一种用于开发和执行网络爬虫的软件工具或库。爬虫框架提供了一组功能和工具，用于自动化地访问网站、下载网页内容、解析HTML数据，并从中提取有用的信息。这些框架通常以模块化的方式设计，使开发人员能够更容易地构建、配置和管理爬虫。\n",
    "\n",
    ":::\n",
    "\n",
    "scrapy 是世界上最流行的爬虫框架之一，它是一个开源的项目，被广泛地运用于爬取各种类型的网站。scrapy 具有十分强大的功能，在本次学习中，我们将从以下几个方面对 scrapy 进行基本的学习。\n",
    "\n",
    "- scrapy的架构、组成部分\n",
    "- 创建和运行一个scrapy爬虫\n",
    "- 使用XPath和CSS选择题提取数据\n",
    "- 编写Spider规则以定义爬取流程\n",
    "\n",
    "现在，让我们从基础入手，来认识一下世界上最棒的爬虫框架 —— scrapy。不过千万要记得，scrapy 作为一个第三方库，在使用它之前，需要安装它。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、scrapy 的架构与组成部分\n",
    "\n",
    "前面我们说过，scrapy具有高度模块化的设计，它包括各种组件，如下载器、中间件、管道、调度器等等，这是 scrapy 的重要优势之一。模块化的设计意味着我们可以根据需要调用我们所需要的组件、使用我们所需要的功能，根据我们的需求灵活地配置和定制爬虫，并且减少资源的占用。\n",
    "\n",
    "scrapy 共有六个核心组件，让我们一起来看看。\n",
    "\n",
    "#### 1. Spiders(蜘蛛)\n",
    "\n",
    "顾名思义，Spiders(蜘蛛) 就是一种 Crawler(爬虫)，在网络世界中，Spiders 会在网页中进行“爬行”和“捕捉(索取）”的工作。Spiders 是 Scrapy 的核心组件，负责定义和爬取的规则与流程。每个 Spider 定义了如何从网站上爬取数据，以及如何解析和处理这些数据。每个Spider定义了从一个或多个起始URL开始的爬取任务，并指定了如何跟踪链接、解析HTML数据以及如何提取有用的信息。就像一只真正的 Spider(蜘蛛)一样，它会从一个起始点(起始网页或种子 URL)开始，沿着蜘蛛网上的线(我们定义好的规则)，追寻和捕捉猎物(跟踪链接和URL，递归地遍历和抓取其他相关网页的内容)。\n",
    "\n",
    "下面让我们一起来看看 Spiders 是如何工作的\n",
    "\n",
    "就像一只小蜘蛛织网一样，Spiders 的主要任务就是定义爬取规则。为了创建一个 Spider，我们首先需要编写一个 Python 类，这个类通常继承自 Scrapy 的 `scrapy.Spider` 类。这个类必须包含一些必要的属性和方法，如 Spider 的名称、起始 URL 列表，以及数据解析逻辑。就像下面这样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['https://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # 在这里编写数据解析逻辑\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于编写数据解析逻辑的方法，我们将在后面学习，现在我们只需要掌握定义一个 Spider 的方法就可以了。\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "**`pass` 关键字**\n",
    "\n",
    "你可能会好奇上面代码中的 `pass` 关键字的作用。在 Python 中，`pass` 是一个占位符关键字，不执行任何操作，只是一个空语句，用于指示此处需要有代码，但在当前情况下不需要执行任何操作。在上面的示例中，我们在 `parse` 方法的内部填上了 `pass` 关键字，用以占位，如果以后我们要编写数据解析逻辑，那么使用新代码替换掉 `pass` 就可以了。\n",
    "\n",
    ":::\n",
    "\n",
    "观察上面的代码，我们在定义这个类时一共做了三件事：\n",
    "\n",
    "- 定义 Spider 的名称：使用 `name` 属性，将其定义为 `myspider`\n",
    "- 定义起始 URL(Start URLs)：每个Spider都有一个或多个起始URL，爬虫将从这些URL开始抓取数据。这些URL可以是单个字符串或URL列表。我们使用 `start_urls` 属性来定义起始 URL。\n",
    "- 定义了一个类方法：定义了 `parse` 方法，向它传入了两个参数：`self` 和 `response`。\n",
    "  - `self`：`self` 参数是一个指向类的实例的引用。你可以这么理解它：当你使用 `self` 时，实际上就是在访问这个类的属性和方法。在上面的代码中，`self` 在 `parse` 方法内部使用，就代表着我们可以在 `parse` 中访问 `Spider` 的属性和方法，如 `name` 和 `start_urls`。`self` 参数就是一个连接到 Spider 对象的“自我引用”。\n",
    "  - `response`：我们在 requests 库的学习中与 `response` 有过一定接触，这个参数在这里是由 Scrapy 框架提供的 HTTP 响应对象，包含了从目标网站下载的页面内容，我们可以使用这个参数中的内容进行提取数据、跟踪链接和执行其他与响应相关的操作。\n",
    "\n",
    "Spiders 作为 Scrapy 最核心的组件，也是我们本次学习所需要掌握的最关键的部分，大家需要牢记创建 Spider 的方法。\n",
    "\n",
    "#### 2. Downloader(下载器)\n",
    "\n",
    "Scrapy 中的 Downloader 是爬虫框架的一个关键组件，该组件负责发送 HTTP 请求以下载网页内容。下载器处理与网络请求和响应相关的底层细节包括处理代理、User-Agent、重定向、Cookie 等。\n",
    "\n",
    "- HTTP 请求发送和响应处理： 下载器负责发送 HTTP 请求，并处理来自服务器的响应。它管理连接池，可以异步发送多个请求以提高效率。\n",
    "- 处理重定向和代理： 下载器处理 HTTP 重定向，以及如果需要的话，可以使用代理服务器来下载页面。这有助于应对某些网站的反爬措施。\n",
    "- 下载网页内容： Downloader 接收到 HTTP 响应后，将网页内容下载到内存中，并将其包装在一个 `Response` 对象中，然后将 `Response` 对象传递给 Spider，以便进一步的处理。我们在创建第一个 Spider 时，在 `parse` 方法里传入的 `response` 参数就是由 Downloader 下载的。\n",
    "\n",
    "#### 3. Item Pipeline(数据管道)\n",
    "\n",
    "Scrapy 的 Item Pipeline 组件负责处理从 Spider 中提取的数据项(Item)，它允许用户再数据被存储或导出之前对数据进行进一步的处理、验证和清洗。我们还可以编写自定义数据管道来处理数据，根据项目的需求实施特定的数据处理逻辑。\n",
    "\n",
    "- 数据清洗：我们可以在 Pipeline 中执行数据清理操作，例如去除不需要的空格、特殊字符或 HTML 标签。\n",
    "- 数据验证：使用 Pipeline 可以验证从 Spider 中提取的数据是否符合特定的格式或要求，如果数据不合规，可以选择丢弃或修复它。\n",
    "- 数据存储：Pipeline 允许将数据保存到不同的存储介质中，例如本地文件、数据库、Elasticsearch 等。\n",
    "- 数据导出：Pipeline 可以将数据导出到不同的输出格式，如 JSON、CSV、XML 等，以便后续分析或共享。\n",
    "- 数据处理：Pipeline 还可以执行其他数据处理任务，例如计算字段、转换数据类型等。\n",
    "\n",
    "在 Scrapy 中，每个 Item Pipeline 是一个独立的处理组件，可以按照优先级的顺序依次应用于提取的数据项。Pipeline 可以配置为启用或禁用，我们可以根据需要启用或禁用不同的 Pipeline 组件。\n",
    "\n",
    "#### 4. Scheduler(调度器)\n",
    "\n",
    "Scheduler 是 Scrapy 框架中的核心组件之一，负责管理爬虫任务的调度和控制。Scheduler的主要任务是维护待爬取的URL队列，确保每个URL都被适时地爬取，并避免重复爬取。\n",
    "\n",
    "- URL 队列管理： Scheduler 负责管理待爬取的 URL 队列。当一个 Spider 启动时，它会从 Scheduler 获取起始 URL，并根据规则和链接跟踪逻辑不断向队列中添加新的 URL。这确保了 Spide 能够按照预定的方式遍历和爬取网站。\n",
    "- 去重处理： 防止重复爬取是爬虫的一个重要任务。Scheduler 会检查新加入队列的 URL 是否已经存在，以避免重复的请求。这可以通过使用去重过滤器(Duplicate Filter)来实现，以确保同一个 URL 不会被爬虫多次请求。\n",
    "- 调度策略： Scheduler 还负责确定下一个要爬取的 URL。它可以使用不同的调度策略，如广度优先、深度优先等，来控制爬虫的行为。调度策略可以根据网站的结构和需求进行配置。\n",
    "- 并发控制： 爬虫通常需要控制并发请求的数量，以避免对目标网站造成过大的负载。Scheduler 允许您配置并发请求数的上限，以便适应目标网站的性能和反爬虫机制。\n",
    "- 错误处理： 当发生网络请求错误或其他异常时，Scheduler 可以处理错误情况，并根据策略重试或忽略失败的请求。这有助于提高爬虫的稳定性。\n",
    "- 分布式爬取： 在分布式爬取环境中，多个爬虫节点可以共享同一个 Scheduler，以协调任务。Scheduler 可以确保不同节点之间不会爬取相同的 URL，从而提高效率。\n",
    "- 自定义扩展： Scrapy 允许开发人员自定义 Scheduler 组件，以满足特定项目的需求。您可以编写自己的调度逻辑，甚至使用外部队列服务(如Redis)来管理 URL 队列。\n",
    "\n",
    "Scheduler 是 Scrapy 中非常重要的组件，它有效地管理了爬虫的调度和 URL 队列，使得爬虫能够高效、有序地访问网站并提取数据。在 Scrapy 中，Scheduler 与 Downloader、Spider 等组件协同工作，构成了一个强大的网络爬虫框架。\n",
    "\n",
    "#### 5. Downloader Middleware(下载器中间件)\n",
    "\n",
    "Downloader Middleware 是用于在 Scrapy 的 Downloader 处理请求和响应的过程中进行拦截和处理的机制。它通常用于执行以下任务：\n",
    "\n",
    "- 请求预处理和响应后处理：下载器中间件是在请求发送之前和响应处理之后执行的一组处理程序。它们可以进行 User-Agent 设置、代理设置、请求头修改、请求过滤、请求重定向处理等任务。\n",
    "- 自定义中间件：我们可以编写自定义的下载器中间件，以满足特定的需求，如处理代理、用户代理切换等。\n",
    "\n",
    "#### 6. Spider Middleware(蜘蛛中间件)\n",
    "\n",
    "Spider Middleware 是用于在Scrapy的 Spider 处理响应数据之前和之后进行拦截和处理的机制。它通常用于执行以下任务：\n",
    "\n",
    "- 请求预处理和响应后处理：蜘蛛中间件是在 Spider 发出请求之前和响应被 Spider 处理之后执行的一组处理程序。它们可以用于全局性的请求预处理和响应后处理，进行数据处理、请求生成、异常处理、数据过滤等任务。\n",
    "- 自定义中间件：我们可以编写自定义的蜘蛛中间件，以实现全局性的功能，如网页去重、异常处理等。\n",
    "\n",
    "### 二、创建第一个 Scrapy 爬虫\n",
    "\n",
    "现在，我们已经对 Scrapy 的构成有了基本的了解。是时候开始上手试一试了，创建一个新的爬虫项目，开始伟大的信息猎人生涯吧。\n",
    "\n",
    "#### 1. 步骤一：创建 Scrapy 项目\n",
    "\n",
    "前面我们已经掌握了如何建立一个新的 Spider，或许你已经心痒难耐要孵化一只你的小虫子了，但是请不要着急，我们现在要使用的是世界上最流行的爬虫框架 Scrapy —— 最流行的东西往往有许多奇奇怪怪的规则，当然，这都是为了我们项目的高效展开。\n",
    "\n",
    "在创建一只 Spider 以前，我们需要首先建立一个 Scrapy 项目。为什么要这么做？因为 Scrapy 作为一个高级 Python 爬虫框架，Scrapy 项目具有一个特定的文件夹结构，其中包含了用于不同方面的代码和配置文件，这个结构包括 Spiders、Item Pipelines、中间件、配置文件等。这个组织结构有助于将爬虫的不同部分分离开来，使其更易于管理和维护。\n",
    "\n",
    "现在让我们来试试看！在 JupyterLab 中输入下面的代码，其中 `DoubanScrapy` 是我们的 Scrapy 项目的名称，你可以自定义一个你喜欢的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'DoubanScrapy', using template directory 'D:\\Programs\\Code Editor and Database\\Anaconda\\envs\\myenv\\Lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    D:\\Files\\Desktop\\爬虫\\DoubanScrapy\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd DoubanScrapy\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject DoubanScrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在瞧一瞧你的目录文件夹，这里应当出现了一个新的文件夹，就是我们刚才创建的 Scrapy 项目保存的位置，请记住它，现在，让我们进入到我们的项目之中。\n",
    "\n",
    "::: {.callout-warning}\n",
    "请注意，在 JupyterLab 中执行的命令会在 JupyterLab 的工作目录中创建或操作文件。如果你想要在 JupyterLab 中创建、运行和管理 Scrapy 项目，确保你已经正确配置了 JupyterLab 的工作目录以便操作项目文件。\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd DoubanScrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经处于我们的项目文件夹下啦，接下来，让我们一起来编辑和开发我们的 Scrapy 项目吧。\n",
    "\n",
    "### 2. 步骤二：创建 Spider\n",
    "\n",
    "前面我们已经学过，如何创建一个 Spider，但是在正式的工作流程中，我们需要使用更通常的办法。就是首先创建一个 Spider，再对它进行编辑。\n",
    "\n",
    "进入到你的 JupyterLab 界面，试试看下面的代码，创建一个新的 Spider，命名为 `douban`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'douban' using template 'basic' \n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider douban movie.douban.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的代码中：\n",
    "\n",
    "- `douban` 是我们所创建的新 Spider 的名称。\n",
    "- `movie.douban.com` 是我们所要爬取的网站的起始URL。\n",
    "\n",
    "上面的命令将在 `DoubanMyScrapy/spiders` 目录下生成一个名为 \"douban.py\" 的 Spider 文件，并将起始 URL 设置为豆瓣电影的页面。\n",
    "\n",
    "### 3. 步骤三：修改 Spider\n",
    "\n",
    "接下来，让我们一起来修改我们的新 Spider。看看你的工作目录，找到 \"douban.py\" 文件，打开它，将其中的内容修改为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class DoubanSpider(scrapy.Spider):\n",
    "    name = \"douban\"\n",
    "    allowed_domains = [\"movie.douban.com\"]\n",
    "    start_urls = [\"http://movie.douban.com/top250\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是将 `start_urls` 修改为 `http://movie.douban.com/top250`。这样一来，我们就完成了对起始 URL 的修改，请务必保证没有修改错误，因为 `start_urls` 是 Spider 在启动时进行爬取的 URL 列表，后续的 URL 都将从初始 URL 获取到的数据中提取。\n",
    "\n",
    "现在你是不是已经着急想要编写爬虫的数据解析逻辑了？请先不要着急，我们接下来还要学习选择器部分，掌握了数据提取的方法，才能更好的编写数据解析逻辑。此处 Spider 数据解析逻辑的编写请让我们先按下不表，后面再来尝试。\n",
    "\n",
    "### 三、数据提取和选择器\n",
    "\n",
    "通过前面的学习，我们很容易就可以获取一个网页的全部数据，但是往往我们不需要那么庞杂的数据。因此，我们需要借助一些工具来将我们所需要的部分从完整的网页数据中提取出来，这种工具就叫做选择器，这个过程就叫做数据提取。\n",
    "\n",
    "Scrapy 提供了两种主要的选择器来提取 HTML 页面中的数据。\n",
    "\n",
    "#### 1. XPth 选择器\n",
    "\n",
    "Scrapy 中的 XPth 选择器是用于爬虫中解析 HTML 或 XML 文档的一种强大工具。在前面学习 HTML 文档时我们知道，HTML 文档由许多标签和元素组成，XPth 选择器就允许根据元素的路径和属性来定位和提取数据，有助于从网页中抽取信息。\n",
    "\n",
    "让我们一起来看看如何使用 XPth 选择器。\n",
    "\n",
    "##### (1) 导入 XPth 选择器\n",
    "\n",
    "在 Scrapy 中，我们需要使用 `Selector` 类来创建 XPth 选择器对象，因此，我们首选需要导入 `Selector` 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.selector import Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) 创建 `Selector` 对象\n",
    "\n",
    "还记得我们在学习创建一个 Spider 时使用 `Pass` 关键字所占下的空位吗？现在我们就要编写代码来替换掉 `Pass` 关键字了。\n",
    "\n",
    "要创建一个 XPth 选择器对象，就要首先将服务器响应给我们的内容(即 `response`)传递给 `Selector` 类的构造函数。在通常情况下，我们都会在 Spider 的 `parse` 方法中使用 XPth 选择器，因此，我们可以这样写："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['https://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        selector = Selector(response)\n",
    "        # 现在可以使用selector来进行XPath选择\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) 选择数据\n",
    "\n",
    "XPath 选择器通过使用 XPath 表达式来选择元素，我们可以使用 XPath 表达式来进行元素选择、层级选择等操作。请回想一下 HTML 文档章节中我们所学习过的标签，在选择数据时，我们需要使用这些标签来做定位。\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "为了便于大家理解，这里不得不提上一句，XPath(XML Path Language)是一种用于在 XML 文档中定位和选择元素的语言，它同样适用于 HTML 文档的解析。XPath 表达式是 XPath 语言中的一种语法，用于指定如何选择文档中的元素。\n",
    "\n",
    ":::\n",
    "\n",
    "1. 元素选择\n",
    "\n",
    "   XPath 表达式中最简单的形式是元素选择，用于选择文档中的元素，其语法为：使用双斜杠 `//` 加上需要选择的元素的标签，如：\n",
    "\n",
    "   - `//p`：选择所有 `<p>` 元素\n",
    "   - `//a`：选择所有 `<a>` 元素\n",
    "\n",
    "2. 层级选择\n",
    "\n",
    "   XPth 表达式可以通过层级结构定位元素，从根节点(父节点)开始逐级选择，各个层级之间使用斜杠 `/` 来分隔，如：\n",
    "\n",
    "   - `/html/body`：选择文档根节点下的所有 `<body>` 元素\n",
    "   - `//div/p`：选择所有 `<div>` 元素下的 `<p>` 元素\n",
    "\n",
    "3. 通配符\n",
    "\n",
    "   通配符 `*` 可以用来选择所有子元素或所有元素。例如：\n",
    "\n",
    "   - 选择`<body>`元素下的所有子元素：`/html/body/*`\n",
    "   - 选择文档中的所有元素：`//*`\n",
    "\n",
    "4. 属性选择\n",
    "\n",
    "   XPath 表达式可以根据元素的属性来选择元素，其语法为：选中某一元素后，在其后紧跟 `[@ ]`，`@` 后填入具体的属性内容。例如：\n",
    "\n",
    "   - 选择所有带有 `href` 属性的`<a>`元素：`//a[@href]`\n",
    "   - 选择所有 `class` 属性为 `header` 的 `<div>` 元素：`//div[@class=\"header\"]`\n",
    "\n",
    "5. 文本选择\n",
    "\n",
    "   XPath 表达式可以选择元素的文本内容。例如：\n",
    "\n",
    "   - 选择所有 `<p>` 元素的文本内容：`//p/text()`\n",
    "   - 选择第一个 `<h1>` 元素的文本内容：`//h1[1]/text()`\n",
    "\n",
    "6. 谓词(Predicates)\n",
    "\n",
    "   谓词用于筛选元素。可以在XPath表达式中使用方括号 `[]` 来定义谓词。例如：\n",
    "\n",
    "   - 选择第一个 `<a>` 元素的文本内容：`//a[1]/text()`\n",
    "   - 选择第一个 `<div>` 元素下的第三个 `<p>` 元素：`//div[1]/p[3]`\n",
    "\n",
    "7. 逻辑运算符\n",
    "\n",
    "   XPath 支持逻辑运算符，如 `and`、`or`、`not`，用于组合条件。例如：\n",
    "\n",
    "   - 选择所有 `<a>` 元素，其文本内容包含 \"click\" 或 \"visit\"：`//a[contains(text(), \"click\") or contains(text(), \"visit\")]`\n",
    "\n",
    "8. 函数\n",
    "\n",
    "   XPath 还包括一系列内置函数，用于在表达式中执行操作，如字符串处理、数学计算等。例如：\n",
    "\n",
    "   - 选择所有文本内容不为空的 `<p>` 元素：`//p[string-length(text()) > 0]`\n",
    "\n",
    "看完了常见的 XPath 表达式，让我们来看看在实际中应该如何书写，要使用 XPath 表达式，只需要调用 `.xpath` 方法就可以了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['https://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        selector = Selector(response)\n",
    "        # 现在可以使用selector来进行XPath选择\n",
    "        # 提取所有链接的文本内容\n",
    "        links = selector.xpath('//a/text()').extract()\n",
    "\n",
    "        # 提取第一个<h1>元素的文本内容\n",
    "        title = selector.xpath('//h1/text()').extract_first()\n",
    "\n",
    "        # 提取所有class属性为\"content\"的<div>元素的文本内容\n",
    "        contents = selector.xpath('//div[@class=\"content\"]/text()').extract()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，我们还可以在一个 XPath 选择器对象上多次进行选择操作。比如下面的例子，我们首先选择所有的链接，然后再选择其文本内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['https://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        selector = Selector(response)\n",
    "        # 现在可以使用selector来进行XPath选择\n",
    "        # 选择所有链接\n",
    "        links = selector.xpath('//a')\n",
    "\n",
    "        # 在选定的链接上选择文本内容\n",
    "        text_contents = links.xpath('./text()').extract()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPath 选择器是 Scrapy 中非常有用的工具，它允许爬虫精确选择和提取数据。熟练掌握 XPath 语法和选择器操作对于成功构建爬虫非常重要。不仅可以提取数据，还可以用于页面导航、链接提取等多个爬虫任务。\n",
    "\n",
    "现在，让我们一起来看看 Scrapy 中的另外一个选择器 —— CSS选择器。\n",
    "\n",
    "#### 2. CSS 选择器\n",
    "\n",
    "回想 HTML 章节的内容，我们知道，网页的三大组成部分分别是：HTML、Javascript 和 CSS。CSS 选择器就是 CSS 中的一部分，它使用类似于CSS的选择器语法来定位和操作元素。CSS 选择器的作用与使用方法与 XPath 选择器类似，让我们一起来试试看！\n",
    "\n",
    "##### (1) 导入 CSS 选择器并创建 `Selector` 对象\n",
    "\n",
    "与 XPath 选择器的操作一致，在 Scrapy 中，要使用 CSS 选择器，就要首先创建 `Selector` 类，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['https://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        selector = Selector(response)\n",
    "        # 现在可以使用selector来进行CSS选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) 选择数据\n",
    "\n",
    "在 Scrapy 中，我们使用 CSS 选择器语法来定位和提取 HTML 或 XML 文旦中的元素。CSS 选择器语法相对简洁，易于理解。同样的，我们也需要牢记 HTML 文档的标签与元素的含义，CSS 选择器也需要借助 HTML 标签来进行数据选择。\n",
    "\n",
    "1. 基本元素选择\n",
    "   \n",
    "   CSS 选择器选择基本元素的语法十分简单，想要选择哪一种元素，就可以直接使用该元素的名称进行选择，例如：\n",
    "\n",
    "   - 选择元素：使用元素名称来选择元素，例如：\n",
    "     - `a` 选择所有链接\n",
    "   - 类选择器：使用类名来选择元素，例如：\n",
    "     - `.classname` 选择所有具有特定类的元素\n",
    "   - ID 选择器：使用 ID 来选择元素，例如：\n",
    "     - `#elementid` 选择具有特定ID的元素\n",
    "\n",
    "2. 层级选择\n",
    "\n",
    "   CSS 选择器与 XPath 选择器一样，同样可以进行层级选择，但是语法不一致：\n",
    "\n",
    "   - 子元素：使用空格来表示子元素，例如：\n",
    "     - `div a` 选择所有 `<div>` 元素内的链接\n",
    "   - 相邻兄弟元素：使用 `+` 符号选择相邻兄弟元素，例如：\n",
    "     - `h1 + p` 选择 `<h1>` 元素后紧随的 `<p>` 元素\n",
    "   - 通用兄弟元素：使用 `~` 符号选择所有通用兄弟元素，例如：\n",
    "     - `h1 ~ p` 选择 `<h1>` 元素后的所有 `<p>` 元素\n",
    "\n",
    "3. 属性选择\n",
    "\n",
    "   - 属性选择：使用方括号来选择具有特定属性的元素，例如：\n",
    "     - `a[href]` 选择所有带有 `href` 属性的链接\n",
    "   - 属性值选择：使用 `[]` 方括号和 `=` 来选择具有特定属性值的元素，例如：\n",
    "     - `input[type=\"text\"]` 选择所有 `type` 属性值为\"text\"的输入框\n",
    "\n",
    "4. 伪类选择器：\n",
    "\n",
    "   CSS选择器还支持伪类选择器，用于选择具有特定状态或位置的元素。例如：\n",
    "\n",
    "   - `:hover` 用于选择鼠标悬停的元素\n",
    "   - `:first-child` 用于选择第一个子元素等\n",
    "\n",
    "5. 组合选择器：\n",
    "\n",
    "   我们可以将多个选择器组合在一起，以便更精确地定位元素。例如：\n",
    "\n",
    "   - `div.content a` 可以选择所有具有类名 \"content\" 的 `<div>` 元素内的链接\n",
    "\n",
    "6. 提取文本和属性：\n",
    "\n",
    "   要提取元素的文本内容，可以使用 `::text`。要提取元素的属性，可以使用 `::attr(attribute_name)`。例如：\n",
    "\n",
    "   - 提取链接文本：`a::text`\n",
    "   - 提取链接的`href`属性：`a::attr(href)`\n",
    "\n",
    "7. 通配符选择器：\n",
    "\n",
    "   使用 `*` 通配符来选择任何元素，例如：\n",
    "\n",
    "   - `*::text` 可以选择文档中的所有文本内容\n",
    "\n",
    "下面给出了一些基本示例，便于大家理解如何使用 CSS 选择器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['https://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        selector = Selector(response)\n",
    "        # 现在可以使用selector来进行CSS选择\n",
    "        # 提取所有链接的文本内容\n",
    "        links = selector.css('a::text').extract()\n",
    "\n",
    "        # 提取第一个`<h1>`元素的文本内容\n",
    "        title = selector.css('h1::text').extract_first()\n",
    "\n",
    "        # 提取所有带有`class`属性为 \"content\"的`<div>`元素的文本内容\n",
    "        contents = selector.css('div.content::text').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到现在，你已经完全掌握了 Scrapy 框架的基础用法，知道了 Scrapy 的架构，学会了如何创建 Spider，了解了如何使用选择器提取数据。让我们来进行本章节学习的最后一步，将编写 Spider 规则、定义爬取流程的工作重新梳理一遍。\n",
    "\n",
    "### 四、编写 Spider 规则以定义爬取流程\n",
    "\n",
    "编写 Spider 规则是定义 Scrapy 爬虫行为的关键部分，它决定了爬虫如何从网页中提取数据和遍历网站的过程。以下是详细描述如何编写 Spider 规则以定义爬取流程的步骤：\n",
    "\n",
    "#### 1. 定义 Spider 类\n",
    "\n",
    "首先，我们需要创建一个继承自 `scrapy.Spider` 类的 Spider 类。在该类中，我们需要定义一些必要的属性，包括 Spider 的名称、起始 URL 列表等。下面，我们一起来创建一个 Spider 示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['https://example.com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 编写 `parse` 方法\n",
    "\n",
    "`parse` 方法是 Spider 的入口点，它接收从起始 URL 下载的响应对象(`response`)。在 `parse` 方法中，我们可以编写数据提取和爬取流程。我们可以使用 XPath 选择器或 CSS 选择器来定位和提取页面中的数据。\n",
    "\n",
    "#### 3. 数据提取和存储：\n",
    "\n",
    "在 `parse` 方法中，我们可以使用选择器从响应中提取数据，然后将数据存储在 Scrapy 的 Item 对象中。Item 对象是字典形式的容器，用于组织和传递数据。\n",
    "\n",
    "::: {.callout-note}\n",
    "在 Scrapy 中，Item 对象是用来存储爬取到的数据的容器。每个 Item 对象都表示一个爬取项，它可以包含从网页中抽取出来的各种数据，如文本、数字、链接、图片URL等。Item 对象是将爬取到的数据传递到 Item Pipeline 的方式之一，以便进行进一步处理、清洗和存储。\n",
    "\n",
    "以下是关于 Scrapy 中 Item 对象的详细介绍：\n",
    "\n",
    "1. 创建 Item 对象：\n",
    "\n",
    "在 Scrapy 中，我们需要定义 Item 对象的结构，这项工作通常是在项目的 `items.py` 文件中进行。一个 Item 对象是一个简单的 Python 类，它通常包含一组字段(field)来存储不同类型的数据。\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class MyItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    description = scrapy.Field()\n",
    "    # 可以添加更多字段\n",
    "```\n",
    "\n",
    "在上述示例中，我们创建了一个名为 `MyItem` 的 Item 类，并定义了三个字段：`title`、`link` 和 `description`。这些字段将用于存储爬取到的数据。\n",
    "\n",
    "2. 传递 Item 对象：\n",
    "\n",
    "一旦 Item 对象包含了所需的数据，我们就可以将其传递给 Spider 中的回调函数，或者直接返回它。 Item 对象将会在 Spider 中传递，然后在 Item Pipeline 中处理。\n",
    "\n",
    "```python\n",
    "def parse(self, response):\n",
    "    item = MyItem()\n",
    "    item['title'] = 'Example Title'\n",
    "    item['link'] = 'https://example.com'\n",
    "    item['description'] = 'This is an example description.'\n",
    "    return item\n",
    "```\n",
    "\n",
    "总之，Item对象是Scrapy中用来存储和传递爬取数据的关键元素。通过创建自定义的Item类并实例化Item对象，我们可以有效地组织和管理从网页中抽取出来的数据，并通过Item Pipeline进行进一步的处理和存储。这使得Scrapy能够高效地处理不同类型的数据，从而实现强大的爬虫功能。\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "#### 4. 使用 `yield` 传递 Item 对象：\n",
    "\n",
    "要将提取的数据传递给 Item Pipeline 进行处理，我们就需要使用 Python 的 `yield` 语句将 Item 对象返回。Scrapy 将自动将返回的 Item 对象传递给配置的 Item Pipeline。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['https://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # 使用XPath选择器提取页面中的标题文本\n",
    "        title = response.xpath('//h1/text()').get()\n",
    "        \n",
    "        # 使用CSS选择器提取页面中的链接\n",
    "        links = response.css('a::attr(href)').getall()\n",
    "        \n",
    "        # 创建一个Item对象并存储提取的数据\n",
    "        item = {\n",
    "            'title': title,\n",
    "            'links': links\n",
    "        }\n",
    "        \n",
    "        # 使用yield将Item对象传递给Item Pipeline进行处理\n",
    "        yield item"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
